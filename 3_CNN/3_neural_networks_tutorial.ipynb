{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vw6iY_35PiZl"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms,datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3YIiFifyPiZq"
   },
   "source": [
    "\n",
    "Neural Networks\n",
    "===============\n",
    "# TODO\n",
    "# networjk on very simple dataset predicting stocks de thymamai kana eukolaki kai meta MNIST\n",
    "# Let's try diffrent networks and setting\n",
    "# PLOT acuracies and loss\n",
    "# Confusion matrix\n",
    "# predictions on sample image\n",
    "# display mnist images\n",
    "# (Optional) use tensorboard for display\n",
    "Use the torch.nn package to build a neural network.\n",
    "We will build two neural networks and try to classify images with digits MNIST dataset.\n",
    "In the last lecture, I have already talked about `` autograd ``, `` nn ``package depends on `` autograd ``\n",
    " package to define the model and get derivative.\n",
    "An ``nn.Module`` contains each layer and a forward (input) method, which returns `` output``.\n",
    "\n",
    "E.g:\n",
    "\n",
    "![](https://pytorch.org/tutorials/_images/mnist.png)\n",
    "\n",
    "It is a simple feed-forward neural network that accepts an input, then passes it layer by layer, and\n",
    "finally outputs the result of the calculation.\n",
    "\n",
    "The typical training process of neural network is as follows:\n",
    "\n",
    "1. Define a neural network model containing some learnable parameters (or weights)\n",
    "\n",
    "1. Iterate over the dataset\n",
    "1. Process input through neural network\n",
    "1. Calculate the loss (the difference between the output and the correct value)\n",
    "1. Parameters of backpropagating the gradient back to the network\n",
    "1. Update the network parameters, mainly using the following simple update principle:\n",
    "``weight = weight - learning_rate * gradient``\n",
    "\n",
    "Create a network:\n",
    "------------------\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "FCNet(\n  (flatten): Flatten()\n  (fc1): Linear(in_features=784, out_features=32, bias=True)\n  (fc2): Linear(in_features=32, out_features=10, bias=True)\n)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FCNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FCNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(28*28, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "    def forward(self,x):\n",
    "        # print('imnput x')\n",
    "        x = F.relu(self.fc1(self.flatten(x)))\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # 1 input image channel, 10 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 10, 5)\n",
    "        self.conv2 = nn.Conv2d(10, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(256, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = FCNet()\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1wRQ_g1_PiZx"
   },
   "source": [
    "The forward function must be defined in the model. The backward function (used to calculate the gradient) is\n",
    "automatically created by ``autograd``. You can use any operation for Tensor in the forward function.\n",
    "\n",
    "``net.parameters()`` returns a list and values of parameters (weights) that can be learned\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "bUMnD_34PiZy",
    "outputId": "e3516476-527e-4bce-bcab-e97d1585f7d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 784])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "len(params)\n",
    "params[0].size() # conv1's .weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZQzS0QMYPiZ2"
   },
   "source": [
    "\n",
    "Note: The expected input size of this network (LeNet) is 32 × 32. If you use the MNIST dataset to train this network,\n",
    "please resize the image to 32 × 32.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "d3axLz4-PiZ3",
    "outputId": "eae85547-53dc-4407-dc59-e58b56556e7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.3378,  0.5920, -0.0722,  0.6209, -0.0205, -0.0661, -0.0077,  0.5482,\n         -0.3803, -0.2409]], grad_fn=<AddmmBackward>)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 28, 28)\n",
    "out = net(input)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "exeUc_1ZPiZ6"
   },
   "source": [
    "Clear the gradient buffer of all parameters to zero, and then perform the back propagation of the random gradient:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "YS_hhwD1PiZ7",
    "outputId": "73a8a3d5-7f6e-423b-ede4-6ca6a172983b"
   },
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MGI6dS4APiZ-"
   },
   "source": [
    "## Note\n",
    " ``torch.nn`` only supports small batch input. The whole `` torch.nn``\n",
    "Packages only support small batch samples, not individual samples.\n",
    "\n",
    "For example, ``nn.Conv2d`` accepts a 4-dimensional tensor,\n",
    "\n",
    "``Each dimension is numSamples * nChannels * Height * Width (number of samples * number of channels * height * width) ``\n",
    "\n",
    "If you have a single sample, just use `` input.unsqueeze (0) `` to add other dimensions \n",
    "\n",
    "Before continuing, let's review the classes used so far.\n",
    "\n",
    "**review:**\n",
    "* `` torch.Tensor``: a used multi-dimensional array * that automatically calls `` backward() `` to\n",
    "support automatic gradient calculation,\n",
    "And save the *gradient* w.r.t about this vector.\n",
    "* `` nn.Module``: neural network module. Package parameters, move to GPU, run, export, load, etc.\n",
    "* `` nn.Parameter``: A variable, when it is assigned to a `` Module ``, it is *automatically registered as a parameter*.\n",
    "* `` autograd.Function ``: To achieve the forward and reverse definition of an automatic derivation operation,\n",
    "each variable operation creates at least one function node, and each `` Tensor `` operation creates and receives one ``Tensor`` and the ``Function`` node of the function that encodes its history.\n",
    "\n",
    "**The key points are as follows:**\n",
    "\n",
    "\n",
    "*    Create a network\n",
    "*    Forward operation of input\n",
    "* Calculate loss then backward operation\n",
    "*    Update network weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Loss function\n",
    "-------------\n",
    "A loss function accepts a pair of (output, target) as input and calculates a value to estimate how much the network\n",
    " output differs from the target value.\n",
    "\n",
    "***Translator's Note: output is the output of the network, and target is the actual value***\n",
    "\n",
    "There are many different [loss functions] in the nn package (https://pytorch.org/docs/nn.html#loss-functions).\n",
    "`` nn.MSELoss `` is a relatively simple loss function, which calculates the **mean square error** between the output\n",
    "and the target,\n",
    "E.g:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4Q5TVnjdPiZ_",
    "outputId": "8870faf8-ac37-401a-eb3f-16244f28735f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4750, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  \n",
    "target = target.view(1, -1)  \n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tAvgGJ2uPiaD"
   },
   "source": [
    "Now, if you follow `` loss`` in the reverse process, use its\n",
    "`` .grad_fn`` attribute, you will see the calculation diagram shown below.\n",
    "\n",
    "::\n",
    "\n",
    "input-> conv2d-> relu-> maxpool2d-> conv2d-> relu-> maxpool2d\n",
    "-> view-> linear-> relu-> linear-> relu-> linear\n",
    "-> MSELoss\n",
    "-> loss\n",
    "\n",
    "So, when we call `` loss.backward () ``, the entire calculation graph will be\n",
    "Differentiate according to loss, and all tensors in the figure set to `` requires_grad = True ``\n",
    "Will have a `` .grad `` tensor that accumulates with the gradient.\n",
    "\n",
    "To illustrate, let us take a few steps back:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "YoCRFbM2PiaE",
    "outputId": "28dce5eb-5557-40e3-cafc-55e69d9aac6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x7f6c61005278>\n",
      "<AddmmBackward object at 0x7f6c61005390>\n",
      "<AccumulateGrad object at 0x7f6c61005278>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fhSbwvIQPiaH"
   },
   "source": [
    "Back propagation\n",
    "--------\n",
    "Call ``loss.backward()`` to get the error of back propagation.\n",
    "\n",
    "However, you need to clear the existing gradient before calling, otherwise the gradient will be accumulated to the\n",
    "existing gradient.\n",
    "Now, we will call ``loss.backward()`` and look at the gradient of the bias term of the conv1 layer before and after\n",
    "back-propagation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "NpSNuUWVPiaH",
    "outputId": "2ea4222a-36a6-4c23-9996-cc63cdd837ee"
   },
   "outputs": [],
   "source": [
    "# net.zero_grad()     \n",
    "\n",
    "# print('conv1.bias.grad before backward')\n",
    "# print(net.conv1.bias.grad)\n",
    "\n",
    "# loss.backward()\n",
    "\n",
    "# print('conv1.bias.grad after backward')\n",
    "# print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dzius6-7PiaM"
   },
   "source": [
    "How to use the loss function\n",
    "\n",
    "**Read later:**\n",
    "\n",
    " The `nn` package contains various modules and loss functions used to form the building blocks of deep neural networks.\n",
    " For complete documentation, please see [here] (https://pytorch.org/docs/nn).\n",
    "\n",
    "\n",
    "\n",
    "Update weights\n",
    "------------------\n",
    "In practice, the simplest weight update rule is stochastic gradient descent (SGD):\n",
    "\n",
    " `` weight = weight-learning_rate * gradient ``\n",
    "\n",
    "We can implement this rule using simple Python code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters ():\n",
    "    f.data.sub_ (f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "But when using a neural network to use various update rules, such as SGD, Nesterov-SGD, Adam, RMSPROP, etc., a package\n",
    "`` torch.optim `` is built in PyTorch to implement all these rules.\n",
    "Using them is very simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t0Tk6cuXPiaM"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NT53M6PwPiaP"
   },
   "source": [
    "**Note:** \n",
    "\n",
    "Observe how to use ``optimizer.zero_grad ()`` to manually set the gradient buffer to zero. This is because the gradient\n",
    "is accumulated as described in the Backprop section.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define our dataloaders\n",
    "\n",
    "We are going to use ``torchvision`` datasets and specifically MNIST dataset to classify images with digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size  = 128\n",
    "#transforms.Normalize(mean=0.5,std=1.0)\n",
    "trans = transforms.Compose([transforms.ToTensor()])\n",
    "training_set = datasets.MNIST(root='./',train=True,transform=trans,download=True)\n",
    "\n",
    "test_set = datasets.MNIST(root='./',train=False,transform=trans,download=True)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=training_set,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch: 0 Loss: 2.32 Acc: 0.18\n",
      " epoch: 0 Loss: 2.29 Acc: 0.26\n",
      " epoch: 0 Loss: 2.25 Acc: 0.33\n",
      " epoch: 0 Loss: 2.16 Acc: 0.37\n",
      " epoch: 0 Loss: 2.03 Acc: 0.42\n",
      " epoch: 1 Loss: 0.82 Acc: 0.78\n",
      " epoch: 1 Loss: 0.70 Acc: 0.80\n",
      " epoch: 1 Loss: 0.63 Acc: 0.82\n",
      " epoch: 1 Loss: 0.59 Acc: 0.83\n",
      " epoch: 1 Loss: 0.56 Acc: 0.83\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "total_epochs = 2\n",
    "log_interval = 100\n",
    "average_loss = 0.\n",
    "net = ConvNet()\n",
    "net = net.to(device)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "for epoch in range(total_epochs):\n",
    "    average_loss = 0.\n",
    "    total_samples  = 0\n",
    "    correct_predictions = 0\n",
    "    for batch_idx, (inputs, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = net(inputs.to(device))\n",
    "       \n",
    "        loss = criterion(outputs,target.to(device))\n",
    "        average_loss+=loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, pred_label = torch.max(outputs.cpu().data, 1)\n",
    "        total_samples += inputs.data.size()[0]\n",
    "        correct_predictions += (pred_label == target.data).sum()\n",
    "        if (batch_idx+1) % 100 == 0 or (batch_idx+1) == len(train_loader):\n",
    "            print(f' epoch: {epoch} Loss: {(average_loss/batch_idx):.2f} Acc: {(correct_predictions * 1.0 / total_samples):.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch: 1, test loss: 0.38, acc: 0.88\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0.\n",
    "total_samples  = 0\n",
    "correct_predictions = 0\n",
    "for batch_idx, (inputs, target) in enumerate(test_loader):\n",
    "    outputs = net(inputs.to(device))\n",
    "    loss = criterion(outputs,target.to(device))\n",
    "    test_loss +=loss.item()\n",
    "    _, pred_label = torch.max(outputs.cpu().data, 1)\n",
    "    total_samples += inputs.data.size()[0]\n",
    "    correct_predictions += (pred_label == target.data).sum()\n",
    "print(f' epoch: {epoch}, test loss: {(test_loss/len(test_loader)):.2f}, acc: {(correct_predictions * 1.0 / total_samples):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IgflFG2gPiaQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "3_neural_networks_tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}