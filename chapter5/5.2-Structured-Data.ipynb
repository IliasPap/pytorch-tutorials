{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "torch.__version__"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5.2 Pytorch handles structured data\n",
    "## Introduction\n",
    "Before the introduction, we must first clarify what is structured data. Structured data, as can be seen from the name, is highly organized and neatly formatted data. It is the type of data that can be placed in tables and spreadsheets. For us, structured data can be understood as a two-dimensional table. For example, a csv file is structured data. It is generally called Tabular Data or structured data in English. Let’s take a look at structured data. example of.\n",
    "\n",
    "The following files are from fastai's own data set:\n",
    "https://github.com/fastai/fastai/blob/master/examples/tabular.ipynb\n",
    "fastai example is here\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data preprocessing\n",
    "The structured data we get is generally a csv file or a table in a database, so for structured data, we can directly use the pasdas library to process it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Read file\n",
    "df = pd.read_csv('./data/adult.csv')\n",
    "#salary is the final classification result of this data set\n",
    "df['salary'].unique()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#View data type\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#pandas's describe can tell us the general structure of the entire data set, which is very useful\n",
    "df.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#View how many data are there in total\n",
    "len(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For model training, only numeric data can be processed, so here we first divide the data into three categories\n",
    "-Training result label: the training result. Through this result, we can clearly know what our training task is, whether it is a classification task or a regression task.\n",
    "-Categorized data: This type of data is discrete and cannot be directly input into the model for training, so we need to process this part first when preprocessing, which is also one of the main tasks of data preprocessing\n",
    "-Numerical data: This type of data can be directly input to the model, but this part of the data may still be discrete, so it can be processed if necessary, and the training accuracy will be greatly improved after processing , Not discussed here"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Training results\n",
    "result_var ='salary'\n",
    "#Sub-type data\n",
    "cat_names = ['workclass','education','marital-status','occupation','relationship','race','sex','native-country']\n",
    "#Numerical data\n",
    "cont_names = ['age','fnlwgt','education-num','capital-gain','capital-loss','hours-per-week']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After manually confirming the data type, we can look at the quantity and distribution of the classification type data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if col in cat_names:\n",
    "        ccol=Counter(df[col])\n",
    "        print(col,len(ccol),ccol)\n",
    "        print(\"\\r\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next step is to convert the sub-type data into numeric data. In this part, we also fill in the missing data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if col in cat_names:\n",
    "        df[col].fillna('---')\n",
    "        df[col] = LabelEncoder().fit_transform(df[col].astype(str))\n",
    "    if col in cont_names:\n",
    "        df[col]=df[col].fillna(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the above code:\n",
    "\n",
    "We first used pandas' fillna function to fill the classified data with null values. It is enough to mark it as a value different from other existing values. The three dashes I used here --- as Mark, and then use sklearn’s LabelEncoder function to process the data\n",
    "\n",
    "Then there is a 0-filling process for our numerical data. For the filling of numerical data, you can also use the average value or fill in other ways. This is not our focus and will not be explained in detail."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After the data processing is completed, you can see that all the data is now digital, and can be directly input to the model for training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Segmentation of training data and labels\n",
    "Y = df['salary']\n",
    "Y_label = LabelEncoder()\n",
    "Y=Y_label.fit_transform(Y)\n",
    "Y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X=df.drop(columns=result_var)\n",
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Above, the basic data preprocessing has been completed. What is shown above is just some necessary processing. If there are many techniques to improve the training accuracy, I won't explain them in detail here.\n",
    "## Define data set\n",
    "To use pytorch to process data, you must use Dataset to define a data set. Define a simple data set below"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class tabularDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.x = X#.to_numpy().astype(float)\n",
    "        self.y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.x.values[idx], self.y[idx])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_ds = tabularDataset(X, Y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can directly use the index to access the data in the defined data set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_ds[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define the model\n",
    "The data has been prepared, the next step is to define our model, here we use a simple model with 3 linear layers as processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class tabularModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(14, 500)\n",
    "        self.lin2 = nn.Linear(500, 100)\n",
    "        self.lin3 = nn.Linear(100, 2)\n",
    "        self.bn1 = nn.BatchNorm1d(14)\n",
    "        self.bn2 = nn.BatchNorm1d(500)\n",
    "        self.bn3 = nn.BatchNorm1d(100)\n",
    "\n",
    "\n",
    "    def forward(self,x_in):\n",
    "        #print(x_in.shape)\n",
    "        x=x_in\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        #print(x)\n",
    "\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        #print(x)\n",
    "\n",
    "        x = self.bn3(x)\n",
    "        x = self.lin3(x)\n",
    "        x=torch.sigmoid(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When defining the model, I saw that we added Batch Normalization to normalize the batch:\n",
    "Please refer to this article for the content of batch normalization: https://mp.weixin.qq.com/s/FFLQBocTZGqnyN79JbSYcQ\n",
    "\n",
    "Or scan this QR code and view it in WeChat:\n",
    "![](https://raw.githubusercontent.com/zergtant/pytorch-handbook/master/deephub.jpg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Specify the equipment used before training\n",
    "DEVICE=torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "        DEVICE=torch.device(\"cuda\")\n",
    "print(DEVICE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Loss function\n",
    "criterion =nn.CrossEntropyLoss().to(DEVICE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Instantiate the model\n",
    "model = tabularModel().to(DEVICE)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Test whether the model is ok\n",
    "rn=torch.rand(3,14).to(DEVICE)\n",
    "model(rn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Learning rate\n",
    "LEARNING_RATE=0.01\n",
    "#BS\n",
    "batch_size = 2048\n",
    "#Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#DataLoaderLoading data\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size,shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above basic steps are required for every training process, so I won’t introduce more, let’s start the model training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "model.train()\n",
    "#Training 10 rounds\n",
    "TOTAL_EPOCHS=10\n",
    "#Record loss function\n",
    "losses = [];\n",
    "for epoch in range(TOTAL_EPOCHS):\n",
    "    for i, (x, y) in enumerate(train_dl):\n",
    "        x = x.float().to(DEVICE) #input must not be float type\n",
    "        y = y.long().to(DEVICE) #The result label must not be of type long\n",
    "        #Clear\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        #Calculate loss function\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.cpu().data.item());\n",
    "        print ('Epoch: %d/%d, Loss: %.4f'%(epoch+1, TOTAL_EPOCHS, loss.data.item()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After the training is complete, we can look at the accuracy of the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "for i,(x, y) in enumerate(train_dl):\n",
    "    x = x.float().to(DEVICE)\n",
    "    y = y.long()\n",
    "    outputs = model(x).cpu()\n",
    "\n",
    "\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += y.size(0)\n",
    "    correct += (predicted == y).sum()\n",
    "print('Accuracy: %.4f %%'% (100 * correct / total))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Through the basic training process, although the accuracy rate has reached 86%, the loss has not dropped at 0.4, indicating that the network is at this level to the greatest extent, so what can be done to improve the accuracy? . Later, more advanced data processing methods will be introduced to improve accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep learning",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}