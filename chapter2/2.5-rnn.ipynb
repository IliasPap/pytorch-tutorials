{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Recurrent Neural Network\n",
    "## 2.5.1 Introduction to RNN\n",
    "One of the biggest characteristics that distinguish our brains from machines is that we have memories and can deduce unknown affairs based on our memories. Our thoughts are persistent. However, the elements of the neural network structure currently introduced in this tutorial are independent of each other, and the input and output are independent.\n",
    "### The cause of RNN\n",
    "In the real world, many elements are connected to each other. For example, the outdoor temperature changes periodically with climate change. Our language also needs to confirm the meaning expressed through context. But it is quite difficult for the machine to do this step. Therefore, there is the current cyclic neural network. Its essence is: it has the ability to remember and will make inferences based on the content of these memories. Therefore, his output depends on the current input and memory.\n",
    "### Why do we need RNN\n",
    "The idea behind RNN is to use sequential information. In traditional neural networks, we assume that all inputs (and outputs) are independent of each other. If you want to predict the next word in a sentence, you need to know which words are in front of it, and you need to see the following words to be able to give the correct answer.\n",
    "RNNs are called loops because they perform the same task on each element of the sequence, and all outputs depend on previous calculations.\n",
    "From another perspective, RNN has a \"memory\" that can capture the information calculated so far. In theory, RNNs can use information in arbitrarily long sequences, but in practice they are limited to reviewing a few steps.\n",
    "The proposal of the cyclic neural network is based on the idea of ​​the memory model. It is expected that the network can remember the previous features and infer the subsequent results based on the features, and the overall network structure continues to circulate because of the name cyclic neural network.\n",
    "\n",
    "\n",
    "### What RNN can do\n",
    "RNN has achieved great success in many NLP tasks. At this point, I should mention that the most commonly used type of RNN is LSTM, which is much better than RNN in capturing long-term dependencies. But don't worry, LSTM is basically the same as the RNN we will develop in this tutorial, they just use a different way to calculate the hidden state. We will introduce LSTM in more detail later. Here are some examples of RNN in NLP:\n",
    "**Language modeling and text generation**\n",
    "\n",
    "Through language modeling, we can generate fake and real text that humans can understand from given words\n",
    "\n",
    "**machine translation**\n",
    "\n",
    "Machine translation is similar to language modeling. We input a series of words in the source language, and the content corresponding to the target language can be output through the calculation of the model.\n",
    "\n",
    "**Speech Recognition**\n",
    "\n",
    "Given the input sequence of acoustic signals from sound waves, we can predict a series of speech fragments and their probabilities, and convert the speech into text\n",
    "\n",
    "**Generate image description**\n",
    "\n",
    "Together with convolutional neural networks, RNN can generate descriptions of unlabeled images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5.2 RNN network structure and principle\n",
    "### RNN\n",
    "The basic structure of the cyclic neural network is particularly simple, that is, the output of the network is stored in a memory unit, and this memory unit enters the neural network together with the next input. We can see that the network will join the memory unit as input when it is input. The network not only outputs the result, but also saves the result in the memory unit. The following figure is a schematic diagram of the simplest recurrent neural network when it is input. [Image source](https://medium.com/explore-artificial-intelligence/an-introduction-to-recurrent-neural-networks-72c97bf0912)\n",
    "\n",
    "![](10.png)\n",
    "RNN can be seen as multiple assignments of the same neural network. Each neural network module will pass the message to the next one. We expand the structure of this graph\n",
    "![](11.png)\n",
    "The network has a cyclic structure, which is the origin of the name of the cyclic neural network. At the same time, according to the structure of the cyclic neural network, it can be seen that it has a natural advantage in processing sequence type data. Because the network itself is a sequence structure, this is also the most essential structure of all recurrent neural networks.\n",
    "\n",
    "Recurrent neural networks have particularly good memory characteristics and can apply memory content to the current situation, but the memory capacity of the network is not as effective as imagined. The biggest problem with memory is that it has forgetfulness. We always remember more recently what happened and forget what happened long ago. Recurrent neural networks also have this problem.\n",
    "\n",
    "The nn.RNN class is used in pytorch to build a sequence-based recurrent neural network. Its constructor has the following parameters:\n",
    "-nput_size: The number of eigenvalues ​​of input data X.\n",
    "-hidden_size: The number of neurons in the hidden layer, which is the number of features in the hidden layer.\n",
    "-num_layers: The number of layers of the recurrent neural network, the default value is 1.\n",
    "-bias: The default is True. If it is false, the neuron does not use the bias parameter.\n",
    "-batch_first: If set to True, the first dimension of the input data dimension is the batch value, and the default is False. By default, the first dimension is the length of the sequence, the second dimension is--batch, and the third dimension is the number of features.\n",
    "-dropout: If it is not empty, it means that the last dropout layer discards part of the data. The percentage of discarded data is specified by this parameter.\n",
    "\n",
    "The most important parameters in RNN are input_size and hidden_size, these two parameters must be clarified. The rest of the parameters usually don't need to be set, just use the default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 32, 50]) torch.Size([2, 32, 50])\n"
     ]
    }
   ],
   "source": [
    "rnn = torch.nn.RNN(20,50,2)\n",
    "input = torch.randn(100, 32, 20)\n",
    "h_0 =torch.randn(2, 32, 50)\n",
    "output,hn=rnn(input ,h_0)\n",
    "print(output.size(),hn.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "LSTM is the abbreviation of Long Short Term Memory Networks, literally translated as Long Short Term Memory Networks. The network structure of LSTM was proposed by Hochreiter and Schmidhuber in 1997, and then this network structure became very popular.\n",
    "Although LSTM only solves the problem of short-term dependence, and it uses deliberate design to avoid the problem of long-term dependence, this approach has proven to be very effective in practical applications. Many people follow up related work to solve many practical problems. , So now LSTM is still widely used. [Image source](https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45)\n",
    "![](lstm.gif)\n",
    "The standard recurrent neural network has only a simple layer structure, while LSTM has 4 layer structures:\n",
    "\n",
    "The first layer is a forget layer: decide what information to discard in the state\n",
    "\n",
    "The second tanh layer is used to generate candidates for updated values, indicating that the state needs to be strengthened in some dimensions and weakened in some dimensions\n",
    "\n",
    "The third layer of sigmoid layer (input gate layer), its output value must be multiplied by the output of the tanh layer, which plays a role of scaling. In extreme cases, the sigmoid output 0 indicates that the state in the corresponding dimension does not need to be updated\n",
    "\n",
    "The last layer determines what to output, and the output value is related to the state. Which part of the candidates will ultimately be output is determined by a sigmoid layer.\n",
    "\n",
    "\n",
    "The nn.LSTM class is used in pytorch to build a sequence-based recurrent neural network. Its parameters are basically similar to RNN, so I won’t list them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 20]) torch.Size([2, 3, 20]) torch.Size([2, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "lstm = torch.nn.LSTM(10, 20,2)\n",
    "input = torch.randn(5, 3, 10)\n",
    "h0 =torch.randn(2, 3, 20)\n",
    "c0 = torch.randn(2, 3, 20)\n",
    "output, hn = lstm(input, (h0, c0))\n",
    "print(output.size(),hn[0].size(),hn[1].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU\n",
    "\n",
    "GRU is short for gated recurrent units and was proposed by Cho in 2014. The biggest difference between GRU and LSTM is that GRU combines the forget gate and the input gate into an \"update gate\". At the same time, the network no longer gives an additional memory state, but the output result is continuously transmitted backwards as a memory state. The input of the network And output becomes particularly simple.\n",
    "\n",
    "![](gru.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 20]) torch.Size([2, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "rnn = torch.nn.GRU(10, 20, 2)\n",
    "input = torch.randn(5, 3, 10)\n",
    "h_0 = torch.randn(2, 3, 20)\n",
    "output, hn = rnn(input, h0)\n",
    "print(output.size(),h0.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5.3 循环网络的向后传播（BPTT）\n",
    "在向前传播的情况下，RNN的输入随着每一个时间步前进。在反向传播的情况下，我们“回到过去”改变权重，因此我们叫它通过时间的反向传播（BPTT）。\n",
    "\n",
    "我们通常把整个序列（单词）看作一个训练样本，所以总的误差是每个时间步（字符）中误差的和。权重在每一个时间步长是相同的（所以可以计算总误差后一起更新）。\n",
    "1. 使用预测输出和实际输出计算交叉熵误差\n",
    "2. 网络按照时间步完全展开\n",
    "3. 对于展开的网络，对于每一个实践步计算权重的梯度\n",
    "4. 因为对于所有时间步来说，权重都一样，所以对于所有的时间步，可以一起得到梯度（而不是像神经网络一样对不同的隐藏层得到不同的梯度）\n",
    "5. 随后对循环神经元的权重进行升级\n",
    "\n",
    "RNN展开的网络看起来像一个普通的神经网络。反向传播也类似于普通的神经网络，只不过我们一次得到所有时间步的梯度。如果有100个时间步，那么网络展开后将变得非常巨大，所以为了解决这个问题才会出现LSTM和GRU这样的结构。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "循环神经网络目前在自然语言处理中应用最为火热，所以后面的内容将介绍一下循环神经网络在处理NLP的时候需要用到的一些其他的知识\n",
    "\n",
    "## 2.5.4 词嵌入（word embedding）\n",
    "在我们人类交流过程中表征词汇是直接使用英文单词来进行表征的，但是对于计算机来说，是无法直接认识单词的。为了让计算机能够能更好地理解我们的语言，建立更好的语言模型，我们需要将词汇进行表征。\n",
    "\n",
    "在图像分类问题会使用 one-hot 编码。比如LeNet中一共有10个数字0-9，如果这个数字是2的话类，它的编码就是 (0，0，1，0， 0，0 ，0，0，0，0)，对于分类问题这样表示十分的清楚，但是在自然语言处理中，因为单词的数目过多比如有 10000 个不同的词，那么使用 one-hot 这样的方式来定义，效率就特别低，每个单词都是 10000 维的向量。其中只有一位是 1 ， 其余都是 0，特别占用内存，而且也不能体现单词的词性，因为每一个单词都是 one-hot，虽然有些单词在语义上会更加接近.但是 one-hot 没办法体现这个特点，所以 必须使用另外一种方式定义每一个单词。\n",
    "\n",
    "用不同的特征来对各个词汇进行表征，相对与不同的特征，不同的单词均有不同的值这就是词嵌入。下图还是来自吴恩达老师的课程截图\n",
    "![](12.png)\n",
    "\n",
    "词嵌入不仅对不同单词实现了特征化的表示，还能通过计算词与词之间的相似度，实际上是在多维空间中，寻找词向量之间各个维度的距离相似度，我们就可以实现类比推理，比如说夏天和热，冬天和冷，都是有关联关系的。\n",
    "\n",
    "在 PyTorch 中我们用 nn.Embedding 层来做嵌入词袋模型，Embedding层第一个输入表示我们有多少个词，第二个输入表示每一个词使用多少维度的向量表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "# an Embedding module containing 10 tensors of size 3\n",
    "embedding = torch.nn.Embedding(10, 3)\n",
    "# a batch of 2 samples of 4 indices each\n",
    "input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])\n",
    "output=embedding(input)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5.5  其他重要概念\n",
    "\n",
    "### Beam search\n",
    "在生成第一个词的分布后，可以使用贪心搜索会根据我们的条件语言模型挑选出最有可能输出的第一个词语，但是对于贪心搜索算法来说，我们的单词库中有成百到千万的词汇，去计算每一种单词的组合的可能性是不可行的。所以我们使用近似的搜索办法，使得条件概率最大化或者近似最大化的句子，而不是通过单词去实现。\n",
    "\n",
    "Beam Search（集束搜索）是一种启发式图搜索算法，通常用在图的解空间比较大的情况下，为了减少搜索所占用的空间和时间，在每一步深度扩展的时候，剪掉一些质量比较差的结点，保留下一些质量较高的结点。虽然Beam Search算法是不完全的，但是用于了解空间较大的系统中，可以减少空间占用和时间。\n",
    "\n",
    "Beam search可以看做是做了约束优化的广度优先搜索，首先使用广度优先策略建立搜索树，树的每层，按照启发代价对节点进行排序，然后仅留下预先确定的个数（Beam width-集束宽度）的节点，仅这些节点在下一层次继续扩展，其他节点被剪切掉。\n",
    "1. 将初始节点插入到list中\n",
    "2. 将给节点出堆，如果该节点是目标节点，则算法结束；\n",
    "3. 否则扩展该节点，取集束宽度的节点入堆。然后到第二步继续循环。\n",
    "4. 算法结束的条件是找到最优解或者堆为空。\n",
    "\n",
    "在使用上，集束宽度可以是预先约定的，也可以是变化的，具体可以根据实际场景调整设定。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意力模型\n",
    "对于使用编码和解码的RNN模型，我们能够实现较为准确度机器翻译结果。对于短句子来说，其性能是十分良好的，但是如果是很长的句子，翻译的结果就会变差。\n",
    "我们人类进行人工翻译的时候，都是一部分一部分地进行翻译，引入的注意力机制，和人类的翻译过程非常相似，其也是一部分一部分地进行长句子的翻译。\n",
    "\n",
    "具体的内容在这里就不详细介绍了\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.5.3 Backward propagation of cyclic network (BPTT)\n",
    "In the case of forward propagation, the input of the RNN advances with each time step. In the case of backpropagation, we \"go back in time\" to change the weights, so we call it Backpropagation Through Time (BPTT).\n",
    "\n",
    "We usually treat the entire sequence (words) as a training sample, so the total error is the sum of the errors in each time step (character). The weight is the same at each time step (so it can be updated together after calculating the total error).\n",
    "1. Calculate cross entropy error using predicted output and actual output\n",
    "2. The network is fully deployed in time steps\n",
    "3. For the expanded network, calculate the gradient of the weight for each practice step\n",
    "4. Because the weights are the same for all time steps, the gradients can be obtained together for all time steps (rather than getting different gradients for different hidden layers like a neural network)\n",
    "5. Then upgrade the weights of circulating neurons\n",
    "\n",
    "The network developed by RNN looks like an ordinary neural network. Backpropagation is also similar to a normal neural network, except that we get the gradient of all time steps at once. If there are 100 time steps, the network will become very huge after unfolding, so in order to solve this problem, structures such as LSTM and GRU will appear.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Recurrent neural networks are currently the most popular in natural language processing, so the following content will introduce some other knowledge that recurrent neural networks need to use when processing NLP\n",
    "\n",
    "## 2.5.4 word embedding\n",
    "In the process of human communication, the characterization vocabulary is directly represented by English words, but for computers, it is impossible to directly recognize words. In order for computers to better understand our language and build a better language model, we need to characterize vocabulary.\n",
    "\n",
    "One-hot encoding is used in image classification problems. For example, there are 10 numbers 0-9 in LeNet. If this number is 2, its code is (0,0,1,0,0,0,0,0,0,0), which is the case for classification problems. The expression is very clear, but in natural language processing, because the number of words is too large, for example, there are 10,000 different words, then the efficiency of using one-hot to define is particularly low. Each word is 10,000-dimensional vector. Only one of them is 1, and the rest are 0, which takes up memory and cannot reflect the part of speech of the word, because every word is one-hot, although some words will be closer in semantics. But one-hot can’t help. To reflect this characteristic, it is necessary to use another way to define each word.\n",
    "\n",
    "Different features are used to characterize each vocabulary. Compared with different features, different words have different values. This is word embedding. The picture below is still a screenshot of the course from teacher Wu Enda\n",
    "![](12.png)\n",
    "\n",
    "Word embedding not only realizes the characteristic representation of different words, but also calculates the similarity between words. In fact, in a multi-dimensional space, we can find the distance similarity of each dimension between word vectors. Analogical reasoning, such as summer and heat, and winter and cold, are all related.\n",
    "\n",
    "In PyTorch, we use the nn.Embedding layer to make the embedding word bag model. The first input of the Embedding layer indicates how many words we have, and the second input indicates how many dimensions of vector representation each word uses."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# an Embedding module containing 10 tensors of size 3\n",
    "embedding = torch.nn.Embedding(10, 3)\n",
    "# a batch of 2 samples of 4 indices each\n",
    "input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])\n",
    "output=embedding(input)\n",
    "print(output.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.5.5 Other important concepts\n",
    "\n",
    "### Beam search\n",
    "After generating the distribution of the first word, we can use greedy search to select the first word most likely to be output according to our conditional language model, but for the greedy search algorithm, there are hundreds of words in our word library. With tens of millions of words, it is not feasible to calculate the possibility of each word combination. Therefore, we use approximate search methods to maximize or approximate the maximum conditional probability of sentences, rather than through words.\n",
    "\n",
    "Beam Search (cluster search) is a heuristic graph search algorithm. It is usually used when the solution space of the graph is relatively large. In order to reduce the space and time occupied by the search, some quality is cut when the depth of each step is expanded. Inferior nodes, keep some higher quality nodes. Although the Beam Search algorithm is incomplete, it can reduce space occupation and time when used to understand systems with larger spaces.\n",
    "\n",
    "Beam search can be regarded as a breadth-first search with constrained optimization. First, a breadth-first strategy is used to build a search tree. At each level of the tree, the nodes are sorted according to the heuristic cost, and then only a predetermined number (Beam width- Cluster width) nodes, only these nodes continue to expand at the next level, and other nodes are cut off.\n",
    "1. Insert the initial node into the list\n",
    "2. The node will be out of the heap. If the node is the target node, the algorithm ends;\n",
    "3. Otherwise, expand the node and take the node of the cluster width into the pile. Then go to the second step to continue the cycle.\n",
    "4. The condition for the end of the algorithm is to find the optimal solution or the heap is empty.\n",
    "\n",
    "In use, the cluster width can be pre-appointed or variable, and the specific settings can be adjusted according to the actual scene.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Attention model\n",
    "For RNN models that use encoding and decoding, we can achieve relatively accurate machine translation results. For short sentences, its performance is very good, but if it is a very long sentence, the translation result will be worse.\n",
    "When we humans do manual translation, we translate part by part. The attention mechanism introduced is very similar to the human translation process, which also translates long sentences part by part.\n",
    "\n",
    "The specific content will not be introduced in detail here\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch 1.0",
   "language": "python",
   "name": "pytorch1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}