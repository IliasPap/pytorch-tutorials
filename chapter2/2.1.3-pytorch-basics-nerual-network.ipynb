{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch basics: neural network package nn and optimizer optm\n",
    "torch.nn is a modular interface specially designed for neural networks. nn is built on Autograd and can be used to define and run neural networks.\n",
    "Here we mainly introduce a few commonly used classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convention: torch.nn For convenience, we will set an alias for him as nn. This chapter has other naming conventions besides nn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First introduce relevant packages\n",
    "import torch\n",
    "# Introduce torch.nn and specify alias\n",
    "import torch.nn as nn\n",
    "#Print the version\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the nn alias, we also quoted nn.functional. This package contains some commonly used functions used in neural networks. The feature of these functions is that they do not have learnable parameters (such as ReLU, pool, DropOut, etc.). These The function can be placed in the constructor or not, but it is not recommended here.\n",
    "\n",
    "Under normal circumstances, we will **set nn.functional to capital F**, so that the abbreviation is convenient to call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a network\n",
    "PyTorch has prepared a ready-made network model for us, as long as it inherits nn.Module and implements its forward method, PyTorch will automatically implement the backward function according to autograd. In the forward function, any function supported by tensor can be used. Use Python syntax such as if, for loop, print, log, etc., and the writing is consistent with the standard Python writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=1350, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        # nn.Module subclass functions must execute the parent class constructor in the constructor\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Convolutional layer '1' means that the input picture is a single channel, '6' means the number of output channels, '3' means that the convolution kernel is 3*3\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        #Linear layer, input 1350 features, output 10 features\n",
    "        self.fc1 = nn.Linear(1350, 10) #How is 1350 calculated here? This depends on the forward function behind\n",
    "    #Forward spread\n",
    "    def forward(self, x):\n",
    "        print(x.size()) # Result: [1, 1, 32, 32]\n",
    "        # Convolution -> Activation -> Pooling\n",
    "        x = self.conv1(x) #According to the calculation formula of the convolution size, the calculation result is 30. The specific calculation formula will be described in detail in the fourth section of the second chapter convolution neural network.\n",
    "        x = F.relu(x)\n",
    "        print(x.size()) # Result: [1, 6, 30, 30]\n",
    "        x = F.max_pool2d(x, (2, 2)) #We use the pooling layer, the calculation result is 15\n",
    "        x = F.relu(x)\n",
    "        print(x.size()) # Result: [1, 6, 15, 15]\n",
    "        # reshape, ‘-1’ means adaptive\n",
    "        #What I did here is the squashing operation, which is to squash the following [1, 6, 15, 15] into [1, 1350]\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        print(x.size()) # Here is the input 1350 of the fc1 layer\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learnable parameters of the network are returned by net.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[-0.1344,  0.1935,  0.0283],\n",
      "          [ 0.1961,  0.2092,  0.2041],\n",
      "          [ 0.2554, -0.1514, -0.1703]]],\n",
      "\n",
      "\n",
      "        [[[-0.0207,  0.0106, -0.1412],\n",
      "          [-0.3179,  0.0231, -0.0267],\n",
      "          [ 0.0117, -0.1489, -0.0665]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2336, -0.2888,  0.0846],\n",
      "          [-0.1015, -0.0979, -0.2875],\n",
      "          [ 0.0362, -0.1150, -0.3182]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2797, -0.3027, -0.1414],\n",
      "          [ 0.0131,  0.0606, -0.3207],\n",
      "          [ 0.0614,  0.2541, -0.1477]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1471,  0.0704,  0.1482],\n",
      "          [ 0.1755, -0.0004,  0.1870],\n",
      "          [-0.2855,  0.1054, -0.3121]]],\n",
      "\n",
      "\n",
      "        [[[-0.0927,  0.3014,  0.2150],\n",
      "          [ 0.3258, -0.2874,  0.1559],\n",
      "          [-0.1763, -0.1845, -0.2128]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3111, -0.2193,  0.3168,  0.1261,  0.2435, -0.1907],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0161, -0.0269,  0.0251,  ..., -0.0108,  0.0045, -0.0151],\n",
      "        [-0.0146,  0.0211, -0.0022,  ..., -0.0183, -0.0070, -0.0070],\n",
      "        [-0.0126, -0.0032, -0.0037,  ...,  0.0204,  0.0197,  0.0060],\n",
      "        ...,\n",
      "        [-0.0208, -0.0015,  0.0009,  ...,  0.0258, -0.0266,  0.0156],\n",
      "        [-0.0011, -0.0103,  0.0095,  ...,  0.0045, -0.0006,  0.0061],\n",
      "        [-0.0088,  0.0224,  0.0039,  ..., -0.0009, -0.0098,  0.0184]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0037, -0.0264, -0.0088, -0.0032, -0.0230, -0.0234, -0.0241, -0.0076,\n",
      "        -0.0169, -0.0011], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for parameters in net.parameters():\n",
    "    print(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "net.named_parameters can return learnable parameters and names at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight : torch.Size([6, 1, 3, 3])\n",
      "conv1.bias : torch.Size([6])\n",
      "fc1.weight : torch.Size([10, 1350])\n",
      "fc1.bias : torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name,parameters in net.named_parameters():\n",
    "    print(name,':',parameters.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input and output of the forward function are both Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "torch.Size([1, 6, 30, 30])\n",
      "torch.Size([1, 6, 15, 15])\n",
      "torch.Size([1, 1350])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32) # The input corresponding to the previous fforward here is 32\n",
    "out = net(input)\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 32, 32])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before backpropagation, first clear the gradient of all parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.ones(1,10)) # The implementation of backpropagation is automatically implemented by PyTorch, we only need to call this function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: torch.nn only supports mini-batches, and does not support inputting one sample at a time, that is, one batch must be entered at a time.\n",
    "\n",
    "In other words, even if we input a sample, the sample will be divided into batches. Therefore, all inputs will increase by one dimension. Let’s compare the input just now. nn is defined as 3 dimensions, but we increase it manually when creating One dimension becomes 4 dimension, the first 1 is batch-size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "In nn, PyTorch also prefabricated the commonly used loss function, below we use MSELoss to calculate the mean square error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.99104881286621\n"
     ]
    }
   ],
   "source": [
    "y = torch.arange(0,10).view(1,10).float()\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(out, y)\n",
    "#loss is a scalar, we can directly use item to get his python type value\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "After calculating the gradients of all parameters in backpropagation, optimization methods are also needed to update the weights and parameters of the network. For example, the update strategy of stochastic gradient descent (SGD) is as follows:\n",
    "\n",
    "weight = weight-learning_rate * gradient\n",
    "\n",
    "Realize most of the optimization methods in torch.optim, such as RMSProp, Adam, SGD, etc. Below we use SGD as a simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "torch.Size([1, 6, 30, 30])\n",
      "torch.Size([1, 6, 15, 15])\n",
      "torch.Size([1, 1350])\n"
     ]
    }
   ],
   "source": [
    "out = net(input) # When called here, the size of x we ​​printed in the forword function will be printed\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(out, y)\n",
    "#Create a new optimizer, SGD only needs to adjust the parameters and learning rate\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr = 0.01)\n",
    "# Clear the gradient first (the same effect as net.zero_grad())\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "#Update parameters\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, a complete dissemination of neural network data has been achieved through PyTorch. The following chapter will introduce the data loading and processing tools provided by PyTorch, which can be used to conveniently process the required data.\n",
    "\n",
    "After reading this section, you may still have doubts about the calculation methods of some parameters in the neural network model. This part will be introduced in detail in Chapter 2, Section 4, Convolutional Neural Networks, and in Chapter 3, Section 2 MNIST Data There are detailed notes in the practical code of handwritten number recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}