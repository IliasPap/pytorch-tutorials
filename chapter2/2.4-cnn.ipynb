{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "print(torch.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.4 Introduction to Convolutional Neural Networks\n",
    "A convolutional neural network consists of one or more convolutional layers and a fully connected layer at the top (1x1 convolutional layers can also be used as the final output) to form a feedforward neural network. It is generally believed that the convolutional neural network was first used in LeNet proposed by Yann LeCun in 1989. However, due to insufficient computing power at that time, it was not widely used. In 1998, Yann LeCun and his collaborators built it. The more complete convolutional neural network LeNet-5 has achieved success in the recognition of handwritten digits. The success of LeNet-5 has attracted attention to the application of convolutional neural networks. LeNet-5 follows the learning strategy of LeCun (1989) and adds a pooling layer to the original design to filter the input features. LeNet-5 basically defines the basic structure of modern convolutional neural networks. The convolutional layer-pooling layer that appears alternately in its construction is considered to effectively extract the translation invariant features of the input image, making the extraction of features a step forward. Stride, so we generally think that Yann LeCun is the founder of convolutional neural networks.\n",
    "\n",
    "After 2006, with the improvement of deep learning theory, especially the improvement of computing power and the emergence of techniques such as fine-tuning, convolutional neural networks began to develop rapidly, deepening their structure, and various types of learning and optimization. The theory has been introduced. AlexNet in 2012, VGGNet in 2014, GoogLeNet and ResNet in 2015 have made convolutional neural networks almost the standard for image processing in deep learning.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.4.1 Why use convolutional neural networks\n",
    "For computer vision, each image is composed of a pixel, and each pixel has three channels, representing the three colors of RGB (without calculating the transparency). Let's take the handwriting recognition data set MNIST as an example, each image It is a monochrome image with a length and width of 28 and a channel of 1. If a fully connected network structure is used, that is, the nerves in the network are connected to each neuron on the adjacent layer, which means our The network has 28 * 28 = 784 neurons (or *3 if RGB3 colors). If 15 neurons are used in the hidden layer, the number of parameters (w and b) required is: 28 * 28 * 15 * 10 + 15 + 10=117625, this order of magnitude is also a terrifying order of magnitude so far. The amount of calculation for a backpropagation is huge. This also shows a monochrome image with a size of 28 pixels. If we use a larger one Pixel, the amount of calculation can be imagined."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.4.2 Structure composition\n",
    "As mentioned above, traditional networks require a large number of parameters, but are these parameters repeated? For example, when we recognize a person, we basically know who this person is by seeing his eyes, nose, mouth, and face. Just use these local features to make a judgment, not all features are needed.\n",
    "Another point is that what we said above can effectively extract the translation invariant features of the input image, as if we saw that this is an eye, whether the glasses are eyes on the left or on the right, this is the translation invariance.\n",
    "We extract the local features of the image through the calculation operation of convolution. Each layer will calculate some local features, and these local features will be summarized to the next layer, so that the features will be transferred from small to large, and finally The image is processed through these local features, which greatly improves the calculation efficiency and accuracy.\n",
    "### Convolutional layer\n",
    "#### Convolution calculation\n",
    "Before introducing the convolutional layer, we must first introduce the calculation of convolution. Here we use a picture on [知胡](https://www.zhihu.com/question/39022858)\n",
    "![](9.gif)\n",
    "We will define a weight matrix, which is what we call W (for convolution, the kernel kernel called convolution is also called filter). The size of this weight matrix is ​​generally `3 * 3` or `5 * 5`, but the larger `7 * 7` is also used in LeNet, which is rare now, because according to experience verification, 3 and 5 are the best sizes.\n",
    "In the way shown in the figure, we use our weight matrix to slide on the input matrix. For each sliding step, the covered value is multiplied by the corresponding value of the matrix, and the result is summed and used as one of the output matrix. Items, and so on until all calculations are completed.\n",
    "\n",
    "As shown in the figure above, our input is a matrix of `5 * 5`, and the calculation result obtained by using a convolution kernel of `3 * 3` is a new matrix of `3 * 3`.\n",
    "So how is the size of the new matrix calculated?\n",
    "#### Convolution kernel size f\n",
    "Just now I mentioned an important parameter, that is the size of the core, we use f here to denote\n",
    "\n",
    "#### Boundary filling (p)adding\n",
    "We see the above picture. After calculation, the size of the matrix has changed. If we want to keep the size of the matrix unchanged, we can fill the matrix first, and surround the matrix by one layer. This matrix becomes ` 7*7`, add 1 to each of the top, bottom, left, and right, which is equivalent to `5+1+1=7`. At this time, the result of the calculation is still a matrix of `5 * 5`, which ensures that the size remains unchanged, where p=1\n",
    "\n",
    "#### Step (s)tride\n",
    "From the animated picture, we can see that each sliding is only one distance, what if each sliding two distances? Then you need to use the step size parameter.\n",
    "\n",
    "#### Calculation formula\n",
    "\n",
    "n is the size of the matrix we input, $ \\frac{n-f+2p}{s} +1 $ rounded down\n",
    "\n",
    "This formula is very important and must be remembered\n",
    "\n",
    "#### Convolutional layer\n",
    "In each convolutional layer, we will set multiple cores, each core represents a different feature, these features are the output we need to pass to the next layer, and our training process is to train these different cores.\n",
    "\n",
    "### Activation function\n",
    "Since the operation of convolution is also linear, it also needs to be activated. In general, relu is used.\n",
    "\n",
    "### Pooling layer (pooling)\n",
    "The pooling layer is an important part of CNN. By reducing the connection between the convolutional layers, the complexity of the operation is reduced. The operation of the pooling layer is very simple. It is equivalent to merging. We enter the size of a filter and convolution The operation is the same, sliding step by step, but the area covered by the filter is merged, and only one value is retained.\n",
    "There are also many ways of merging, for example, we commonly use the two maximum values ​​of maxpooling and the average value of avgpooling.\n",
    "\n",
    "The output size formula of the pooling layer is also the same as that of the convolutional layer. Since there is no padding, p=0, which can be simplified to\n",
    "$ \\frac{n-f}{s} +1 $\n",
    "\n",
    "### dropout layer\n",
    "Dropout is a trick used by Hinton in 2014 to prevent overfitting, which enhances the generalization ability of the model\n",
    "Dropout (random deactivation) refers to temporarily discarding part of the neural network unit from the network according to a certain probability during the training process of the deep learning network, which is equivalent to finding a thinner network from the original network, which is a bit more popular. , Which is to randomly cut off part of the network's spread. It sounds unreliable, but the actual test results are very good.\n",
    "If you are interested, you can read the original text [Dropout: A Simple Way to Prevent Neural Networks from Overfitting] (http://jmlr.org/papers/v15/srivastava14a.html) here is not introduced in detail.\n",
    "\n",
    "### Fully connected layer\n",
    "The fully connected layer is generally used as the final output layer. The function of convolution is to extract the features of the image. The final fully connected layer is to calculate through these features and output the results we want, whether it is classification or regression.\n",
    "\n",
    "Our features are all represented by matrices, so we need to flatten the features before passing them into the fully connected layer, and turn these features into one-dimensional vectors. If we want to classify, we use sofmax as the output. If If it returns, just use linear directly."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above are the main components of the convolutional neural network. Below we introduce some classic network models\n",
    "## 2.4.3 Classic Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LeNet-5\n",
    "1998, Yann LeCun’s LeNet5 [Official Website](http://yann.lecun.com/exdb/lenet/index.html)\n",
    "\n",
    "The pioneering work of convolutional neural networks. Although the sparrow is small, it has all five internal organs. Convolutional layers, pooling layers, and fully connected layers are all basic components of modern CNN networks.\n",
    "   -Use convolution to extract spatial features;\n",
    "   -Obtain sub-samples by spatial averaging;\n",
    "   -Use tanh or sigmoid to get nonlinearity;\n",
    "   -Use multi-layer neural network (MLP) as the final classifier;\n",
    "   -Use a sparse connection matrix between layers to avoid large computational costs.\n",
    "![](lenet5.jpg)\n",
    "\n",
    "Input: The image size is 32*32. This is larger than the largest letter (28*28) in the mnist database. The purpose of this is to hope that potential obvious features, such as intermittent strokes and corner points, can appear in the center of the highest-level feature monitoring sub-receptive field.\n",
    "\n",
    "Output: 10 categories, respectively, the probability of 0-9 numbers\n",
    "\n",
    "1. The C1 layer is a convolutional layer with 6 convolution kernels (6 types of local features are extracted), and the kernel size is 5 * 5\n",
    "2. The S2 layer is the pooling layer, and downsampling (area: 2 * 2) reduces the network training parameters and the degree of overfitting of the model.\n",
    "3. The C3 layer is the second convolutional layer, using 16 convolution kernels, kernel size: 5 * 5 to extract features\n",
    "4. The S4 layer is also a pooling layer, area: 2*2\n",
    "5. The C5 layer is the last convolution layer, the size of the convolution kernel: 5 * 5 types of convolution kernel: 120\n",
    "6. Finally, use the fully connected layer to classify the 120 features of C5, and finally output the probability of 0-9\n",
    "\n",
    "The following code is from [Official Tutorial](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class LeNet5(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120) # The paper here is conv, and the official tutorial uses the linear layer\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = LeNet5()\n",
    "print(net)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### AlexNet\n",
    "2012, Alex Krizhevsky\n",
    "Can be counted as a deeper and broader version of LeNet, which can be used to learn more complex objects [Paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural -networks.pdf)\n",
    "   -Use rectified linear units (ReLU) to get nonlinearity;\n",
    "   -Use the dropout technique to selectively ignore individual neurons during training to slow down the model's overfitting;\n",
    "   -Overlap the largest pool to avoid the average effect of the average pool;\n",
    "   -Using GPU NVIDIA GTX 580 can reduce training time, which is 10 times faster than processing with CPU, so it can be used for larger data sets and images.\n",
    "![](alexnet.png)\n",
    "Although AlexNet has only 8 layers, it has a total of more than 60M parameters. Alexnet has a special calculation layer, LRN layer, which does smoothing of the output results of the current layer. I won’t introduce it in detail here.\n",
    "Each stage of Alexnet (including the main calculation of a convolution is counted as one layer) can be divided into 8 layers:\n",
    "1. con-relu-pooling-LRN:\n",
    "It should be noted that the input layer is 227*227 instead of 224 in the paper. Here you can calculate it, mainly because 227 can be divisible by the following conv1 calculation, and 224 is not divisible. If you must use 224, it can be realized by automatic edge supplement, but it feels meaningless to fill the edge in the input, and the complement is also 0. This is the importance of the formula we mentioned above.\n",
    "\n",
    "2. conv-relu-pool-LRN:\n",
    "group=2, this attribute forcibly separates the feature map of the previous result, and the convolution part is divided into two parts\n",
    "\n",
    "3. conv-relu\n",
    "\n",
    "4. conv-relu\n",
    "\n",
    "5. conv-relu-pool\n",
    "\n",
    "6. fc-relu-dropout:\n",
    "The dropout layer, in alexnet, means that during training, the output of some neurons of the hidden layer is 0 with a probability of 1/2, so that half of the node output is lost, and these nodes are not updated during BP to prevent overfitting .\n",
    "\n",
    "7. fc-relu-dropout\n",
    "\n",
    "8. fc-softmax\n",
    "\n",
    "The vision package of Pytorch contains the official implementation of Alexnet, we directly use the official version to see the network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torchvision\n",
    "model = torchvision.models.alexnet(pretrained=False) #We do not download pre-training weights\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### VGG\n",
    "In 2015, VGG in Oxford. [Paper](https://arxiv.org/pdf/1409.1556.pdf)\n",
    "\n",
    "   -Use smaller 3×3 filters in each convolutional layer and combine them into a convolutional sequence\n",
    "   -Multiple 3×3 convolution sequences can simulate the effect of a larger receiving field\n",
    "   -Each time the image pixels are reduced by one time, the number of convolution kernels is doubled\n",
    "\n",
    "There are many versions of VGG, which can be regarded as a relatively stable and classic model. It is also characterized by a huge amount of continuous conv and multiple calculations. Here we take VGG16 as an example. [Image source](https://www.cs.toronto.edu/~frossard/post/vgg16/)\n",
    "![](vgg16.png)\n",
    "VGG uses only small convolution kernels, combined with the author's and his own opinions, here are the advantages of small convolution kernels than using large convolution kernels:\n",
    "\n",
    "According to the author's point of view, after input8 -> layer 3 conv3x3, output=2, which is equivalent to the result of layer 1 conv7x7; input=8 -> layer 2 conv3x3, output=2, which is equivalent to the result of layer 2 conv5x5\n",
    "\n",
    "The parameters of the convolutional layer are reduced. Compared with the large convolution kernels of 5x5, 7x7 and 11x11, 3x3 significantly reduces the amount of parameters\n",
    "\n",
    "After passing through the convolution and pooling layers, the resolution of the image is reduced to half of the original, but the characteristics of the image are doubled, which is a very regular operation:\n",
    "The resolution is input by 224->112->56->28->14->7,\n",
    "Features from the original RGB 3 channels -> 64 -> 128 -> 256 -> 512\n",
    "\n",
    "This provides a standard for the following network, we still use the official version of Pytorch to view"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torchvision\n",
    "model = torchvision.models.vgg16(pretrained=False) #We do not download pre-training weights\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GoogLeNet (Inception)\n",
    "2014, Google Christian Szegedy [Paper](https://arxiv.org/abs/1512.00567)\n",
    "-Use 1×1 convolutional block (NiN) to reduce the number of features, which is usually referred to as the \"bottleneck\", which can reduce the computational burden of deep neural networks.\n",
    "-Before each pooling layer, add feature maps and increase the width of each layer to increase the combination of features\n",
    "\n",
    "The biggest feature of googlenet is that it contains several inception modules, so it is sometimes called inception net.\n",
    "Although googlenet has a lot more layers than VGG, the calculation speed is much faster due to the design of inception.\n",
    "\n",
    "![](googlenet.png)\n",
    "\n",
    "Don’t be scared by this picture, the principle is actually very simple\n",
    "\n",
    "The main idea of ​​the Inception architecture is to find out how to make the existing dense components close to and cover the best local sparse structure in the convolutional visual network. Now you need to find the optimal local structure and repeat it several times. A previous document proposed a layer-to-layer structure, and carried out correlation statistics on the last layer to gather high correlations together. These clusters form the units of the next layer and are connected to the units of the upper layer. Assuming that each unit of the previous layer corresponds to certain areas of the input image, these units are divided into filter banks. In the lower layer close to the input layer, the relevant units are concentrated in some local areas, and finally a large number of clusters in a single area are obtained, which are covered by 1x1 convolution in the last layer.\n",
    "\n",
    "The above words sound very blunt, but the explanation is very simple: we use several different feature extraction methods for each module, such as 3x3 convolution, 5x5 convolution, 1x1 convolution, pooling, etc., calculate them all, and finally Then connect these results through Filter Concat to find the most effective one. And the network contains many such modules, so we don't need to manually judge which feature extraction method is good, the network will solve it by itself (is it a bit like AUTO ML), InceptionA-E is implemented in Pytorch, and the InceptionAUX module.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# inception_v3 requires scipy, so if it is not installed, pip install scipy\n",
    "import torchvision\n",
    "model = torchvision.models.inception_v3(pretrained=False) #We do not download pre-training weights\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ResNet\n",
    "2015, Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun [Paper](https://arxiv.org/abs/1512.03385)\n",
    "Kaiming He He Kaiming (transliteration) everyone must remember that he is involved in many papers (mask rcnn, focal loss). Needless to say, Mr. Jian Sun is the chief scientist of Questyle Technology.\n",
    "The googlenet just now is very deep, and ResNet can do it deeper. Through residual calculation, it can train a network with more than 1000 layers, commonly known as jump connection\n",
    "\n",
    "#### Degradation problem\n",
    "The number of network layers increased, but the accuracy on the training set was saturated or even decreased. This cannot be interpreted as overfitting, because overfit should behave better on the training set. This is the problem of network degradation. The degradation problem shows that deep networks cannot be easily optimized.\n",
    "\n",
    "#### Solution to residual network\n",
    "The latter layers of the deep network are identity mappings, then the model degenerates into a shallow network. The problem to be solved now is to learn the identity mapping function. It is more difficult to let some layers fit a potential identity mapping function H(x) = x. If the network is designed as H(x) = F(x) + x. We can convert to learning a residual function F(x) = H(x)-x. As long as F(x)=0, it constitutes an identity mapping H(x) = x. Moreover, it is definitely easier to fit the residuals.\n",
    "\n",
    "The above is very difficult to understand, continue to explain, first look at the picture:\n",
    "![](resnet.png)\n",
    "\n",
    "We add the output of the previous layer (or several layers) to the output calculated in this layer before the activation function, input the result of the sum into the activation function as the output of this layer, and introduce the residual mapping to the output The change is more sensitive, in fact, it is to see whether there is a big change in this layer relative to the previous layers, which is equivalent to the role of a differential amplifier. The curve in the figure is the shoutcut in the residual. He directly connects the result of the previous layer to this layer, which is commonly known as jump connection.\n",
    "\n",
    "Let’s take a look at the network structure with the classic resnet18 [Image source](https://www.researchgate.net/figure/Proposed-Modified-ResNet-18-architecture-for-Bangla-HCR-In-the-diagram-conv -stands-for_fig1_323063171)\n",
    "![](resnet18.jpg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torchvision\n",
    "model = torchvision.models.resnet18(pretrained=False) #We do not download pre-training weights\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So how do we choose the network?\n",
    "[Source](https://www.researchgate.net/figure/Comparison-of-popular-CNN-architectures-The-vertical-axis-shows-top-1-accuracy-on_fig2_320084139)\n",
    "![](cnn.png)\n",
    "The above table can clearly see the comparison between accuracy rate and calculation amount. My suggestion is that resnet18 is basically available for small image classification tasks. If the accuracy requirements are really high, then choose another better network architecture.\n",
    "\n",
    "**Another saying goes: the poor can only use alexnet, the rich use Res**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}