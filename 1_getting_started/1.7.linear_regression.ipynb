{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Bn6a7tTaR9w",
    "outputId": "02ce2649-5a84-4431-e503-b678a3e36f42"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x7f727f708d08>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "torch.__version__\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Z6sEB4YaR9-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Linear Regression\n",
    "\n",
    "useful links\n",
    "![Linear_regression_img](./figures/1.7.linear_regression.png)\n",
    "\n",
    "https://www.youtube.com/watch?v=zPG4NjIkCjc\n",
    "\n",
    "https://www.kaggle.com/aakashns/pytorch-basics-linear-regression-from-scratch\n",
    "\n",
    "Linear regression models a linear relationship between two variables. There is usually an independent value $x$\n",
    "and a dependent value $y$. Linear regression has an equation with the form $y=ax+b$ and finds the optimal values\n",
    "$a$ and $b$ that best describe the relationship of the variables. More specifically, this equation describes a straight\n",
    "line with slope eaual to $a$ and $b$ the intercept (the value of $y$ when $x = 0$).\n",
    "\n",
    "\n",
    "\n",
    "Let's create and initialize randomly our model's variables  $a$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UgQ5q7oTaR9_",
    "outputId": "4ff1d2b6-8d49-420a-d81f-3b544000fbd6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.5410], requires_grad=True)\n",
      "tensor([-0.2934], requires_grad=True)\n",
      "tensor([-3.6509], grad_fn=<AddBackward0>) tensor([-2.1788])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a = torch.randn(1,requires_grad=True)\n",
    "b = torch.randn(1,requires_grad=True)\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "\n",
    "x = torch.randn(1)\n",
    "y = a*x+b\n",
    "\n",
    "y.backward()\n",
    "print(y,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IKMOmY6EaR-J",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Fit simple line\n",
    "\n",
    "Let's create dummy data and try to fit our linear regression model. We'll initialize randomly our a,b and try to run\n",
    "some iterations to find the optimal weights that fi oour following line.\n",
    "\n",
    "$y=2x+0.5$\n",
    "\n",
    "Now let's create our data and  fit our model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zJRBkGbRaR-L",
    "outputId": "27ca2ebc-f61b-4b6a-b33b-90dbaf4b915a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "a = torch.randn((1,1),requires_grad=True)\n",
    "b = torch.randn(1,requires_grad=True)\n",
    "\n",
    "def model(x):\n",
    "    return x @ a.t() + b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MSE LOSS\n",
    "Mean Squared Error (MSE) or mean squared deviation (MSD) of an estimator\n",
    "(of a procedure for estimating an unobserved quantity) measures the average of the squares of the errors.\n",
    "\n",
    "$MSE(y,\\hat {y}) =\\sum_{i=1}^{N} (y_{i}-\\hat{y}_{i})^{2} $\n",
    "\n",
    "In other words MSE is the mean ${ \\left({\\frac {1}{n}}\\sum _{i=1}^{n}\\right)}$\n",
    "of the squares of the errors ${ (y_{i}-{\\hat {y_{i}}})^{2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def mse(y,y_hat):\n",
    "     return((y-y_hat)**2).mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Create dataset\n",
    "Now we'll create our data that decribe the equation $y=2x+0.5$.\n",
    "We will create only 10 samples but you can do more if you like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1])\n",
      "tensor([[ 1.],\n",
      "        [ 2.],\n",
      "        [ 3.],\n",
      "        [ 4.],\n",
      "        [ 5.],\n",
      "        [ 6.],\n",
      "        [ 7.],\n",
      "        [ 8.],\n",
      "        [ 9.],\n",
      "        [10.]])\n",
      "torch.Size([10, 1])\n",
      "tensor([[ 2.5000],\n",
      "        [ 4.5000],\n",
      "        [ 6.5000],\n",
      "        [ 8.5000],\n",
      "        [10.5000],\n",
      "        [12.5000],\n",
      "        [14.5000],\n",
      "        [16.5000],\n",
      "        [18.5000],\n",
      "        [20.5000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iliask/Desktop/ilias/python_envs/pytorch-tutorials/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.range(1,10).float().unsqueeze(-1)\n",
    "print(inputs.shape)\n",
    "print(inputs)\n",
    "targets = 2. * inputs + 0.5 * torch.ones(10,1)\n",
    "print(targets.shape)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's predict $\\hat{y}$ with the untrained model and see what the output and loss values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5161],\n",
      "        [ 0.0523],\n",
      "        [ 0.6208],\n",
      "        [ 1.1892],\n",
      "        [ 1.7576],\n",
      "        [ 2.3261],\n",
      "        [ 2.8945],\n",
      "        [ 3.4629],\n",
      "        [ 4.0314],\n",
      "        [ 4.5998]], grad_fn=<AddBackward0>)\n",
      "tensor(106.3641, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "print(preds)\n",
    "\n",
    "\n",
    "# Compute loss\n",
    "loss = mse(preds, targets)\n",
    "print(loss)\n",
    "\n",
    "# Compute gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now if we do one backpropagation step, the gradients of the two parameters $a$ and $b$ we will be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5684]], requires_grad=True)\n",
      "tensor([[-127.6605]])\n",
      "tensor([-1.0845], requires_grad=True)\n",
      "tensor([-18.9163])\n",
      "tensor([[0.]])\n",
      "tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "# Gradients for weights\n",
    "print(a)\n",
    "print(a.grad)\n",
    "\n",
    "\n",
    "\n",
    "# Gradients for bias\n",
    "print(b)\n",
    "print(b.grad)\n",
    "\n",
    "\n",
    "\n",
    "a.grad.zero_()\n",
    "b.grad.zero_()\n",
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "245_6rwPaR-T",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's train the model now for 100 iterations and test again the predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SGn0aA7SaR-U",
    "outputId": "947c5d8b-d48f-4ad5-a2fd-63f23e879e2c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Done\n",
      "a = tensor([[2.1856]], requires_grad=True) b = tensor([-0.7955], requires_grad=True)\n",
      "Predictions tensor([[ 1.3901],\n",
      "        [ 3.5757],\n",
      "        [ 5.7614],\n",
      "        [ 7.9470],\n",
      "        [10.1326],\n",
      "        [12.3182],\n",
      "        [14.5039],\n",
      "        [16.6895],\n",
      "        [18.8751],\n",
      "        [21.0607]], grad_fn=<AddBackward0>)\n",
      "Loss = 0.3596566319465637\n"
     ]
    }
   ],
   "source": [
    "# Train for 100 epochs\n",
    "lr = 1e-3\n",
    "for i in range(100):\n",
    "    preds = model(inputs)\n",
    "    \n",
    "    loss = mse(preds, targets)\n",
    "    \n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        a -= a.grad * lr\n",
    "        b -= b.grad * lr\n",
    "        a.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "\n",
    "print('Optimization Done')\n",
    "print(f'a = {a} b = {b}')\n",
    "\n",
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "print(f'Predictions {preds}')\n",
    "\n",
    "\n",
    "# Compute loss\n",
    "loss = mse(preds, targets)\n",
    "print(f'Loss = {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ly44eqE8aR-a",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Run linear regression with multidimensional data\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "y =\n",
    "\\left[ \\begin{array}{cc}\n",
    "x_{11} & x_{12} &x_{13} \\\\\n",
    "x_{21} & x_{22} &x_{23} \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "x_{N1} & x_{N2} &x_{N3}\n",
    "\\end{array} \\right]\n",
    "%\n",
    "\\times\n",
    "%\n",
    "\\left[ \\begin{array}{cc}\n",
    "w_{11} & w_{21} \\\\\n",
    "w_{12} & w_{22} \\\\\n",
    "w_{13} & w_{23}\n",
    "\\end{array} \\right]\n",
    "%\n",
    "+\n",
    "%\n",
    "\\left[ \\begin{array}{cc}\n",
    "b_{1} & b_{2} \\\\\n",
    "b_{1} & b_{2} \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "b_{1} & b_{2} \\\\\n",
    "\\end{array} \\right]\n",
    "$$\n",
    "\n",
    "Wine Quality dataset\n",
    "\n",
    "We will try to solve a real problem now instead of using random data. We will use the linear regression model to\n",
    "predict the wine quality based on different metrichs (pH, acidity, etc.)\n",
    "You can download the dataset from the following link,\n",
    "![Dataset link]https://archive.ics.uci.edu/ml/datasets/Wine+Quality,\n",
    "Let's read our dataset now and explore what type of data it contains.\n",
    "\n",
    "![wine_data](./figures/1.7.data_viz.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qiMyQgm8aR-c",
    "outputId": "969a16a7-bfe4-475b-9c06-6b5677f648b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_csv.reader object at 0x7f71f15d8c18>\n",
      "torch.Size([1599, 11])\n",
      "torch.Size([1599, 1])\n",
      "tensor([[ -8.8427],\n",
      "        [-12.6074],\n",
      "        [ -8.7062],\n",
      "        ...,\n",
      "        [-17.3114],\n",
      "        [-17.4414],\n",
      "        [-10.6283]], grad_fn=<AddBackward0>)\n",
      "tensor(319.4118, grad_fn=<MeanBackward0>)\n",
      "tensor([[-1.3986,  0.4033,  0.8380, -0.7193, -0.4033, -0.5966,  0.1820, -0.8567,\n",
      "          1.1006, -1.0712,  0.1227]], requires_grad=True)\n",
      "tensor([[ -289.8227,   -17.3111,    -9.7031,   -89.3588,    -2.9629,  -579.3581,\n",
      "         -1439.5347,   -33.7111,  -111.6209,   -22.5599,  -354.4187]])\n",
      "tensor([-0.5663], requires_grad=True)\n",
      "tensor([-33.8159])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def read_wine_data():\n",
    "    with open('./data/winequality-red.csv') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=';')\n",
    "        print(csv_reader)\n",
    "        line_count = 0\n",
    "        wine_data = []\n",
    "        categories = []\n",
    "        for idx,row in enumerate(csv_reader):\n",
    "            #print(row)\n",
    "            if idx ==0 :\n",
    "                categories = row\n",
    "            else:\n",
    "                r = list(map(float, row))\n",
    "                wine_data.append(r)\n",
    "        # Convert inputs and targets to tensors\n",
    "        data_tensor = torch.tensor(wine_data)\n",
    "    return data_tensor,categories\n",
    "\n",
    "\n",
    "a = torch.randn(1, 11, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "data_tensor,categories = read_wine_data()\n",
    "inputs = data_tensor[:,:-1]\n",
    "targets = data_tensor[:,-1].unsqueeze(-1)\n",
    "print(inputs.shape)\n",
    "print(targets.shape)\n",
    "\n",
    "\n",
    "\n",
    "# Define the model\n",
    "def model(x):\n",
    "    return x @ a.t() + b\n",
    "\n",
    "# MSE loss\n",
    "\n",
    "def mse(y,y_hat):\n",
    "     return((y-y_hat)**2).mean()\n",
    "\n",
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "print(preds)\n",
    "\n",
    "# Compute loss\n",
    "loss = mse(preds, targets)\n",
    "print(loss)\n",
    "\n",
    "# Compute gradients\n",
    "loss.backward()\n",
    "\n",
    "# Gradients for weights\n",
    "print(a)\n",
    "print(a.grad)\n",
    "\n",
    "# Gradients for bias\n",
    "print(b)\n",
    "print(b.grad)\n",
    "\n",
    "a.grad.zero_()\n",
    "b.grad.zero_()\n",
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EmzTOIWhaR-i",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's do  100 optimization iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z2iPT4vZaR-j",
    "outputId": "d49ad10f-b8be-4759-c5e2-887c9c9cd480",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 102.35\n"
     ]
    }
   ],
   "source": [
    "# Train for 100 epochs\n",
    "lr = 1e-5\n",
    "for i in range(100):\n",
    "    preds = model(inputs)\n",
    "    loss = mse(preds, targets)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        a -= a.grad * lr\n",
    "        b -= b.grad * lr\n",
    "        a.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "\n",
    "print(f'Loss {loss.item():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rgSo-5BPaR_A",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define Linear Regression model using PyTorch built in Functions\n",
    "\n",
    "Now, we are going to reimplement the same model using PyTorch built-in libraries.\n",
    "To create a linear model we will use Linear() class from torch.nn package.\n",
    "To calculate MSE loss we will import nn.MSELoss() and torch.optim.SGD to\n",
    "create a stochastic gradient descent optimizer to our model.\n",
    "These are the main steps to do\n",
    "- Read Data\n",
    "- Create Dataloader\n",
    "- Create Model, Optimizer and Loss Functions\n",
    "- Train the model\n",
    "- Test the model\n",
    "\n",
    "Let's train again our classifier to predict the linear relationship $y=2x+0.5$\n",
    "and then we are going to classify again the wine dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CcWrHh98aR_A",
    "outputId": "b8d38193-00c0-4b41-f9d2-85631986dbac",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.14781543612480164\n",
      "tensor([[1.8806]])\n",
      "tensor([1.2102])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iliask/Desktop/ilias/python_envs/pytorch-tutorials/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.arange(0,10).float().unsqueeze(-1)\n",
    "\n",
    "targets = 2. * torch.arange(0,10).float().unsqueeze(-1) + 0.5*torch.ones(10,1)\n",
    "\n",
    "lr_model = nn.Linear(in_features=1,out_features=1)\n",
    "optimizer = torch.optim.SGD(lr_model.parameters(),lr=0.001)\n",
    "criterion = nn.MSELoss(size_average=True)\n",
    "\n",
    "for i in range(100):\n",
    "    preds = lr_model(inputs)\n",
    "    loss = criterion(preds,targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "print(f'Loss {loss.item()}')\n",
    "print(lr_model.weight.data)\n",
    "print(lr_model.bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LxnEmMqOaR_H",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's classify again our wine dataset with the PyTorch buit-in functions now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2PxuI4NSaR_I",
    "outputId": "00393919-4b01-4832-9e6b-d768da60fe4e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_csv.reader object at 0x7f71f14a2278>\n",
      "torch.Size([1599, 12])\n",
      "['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']\n",
      "tensor([[5.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [6.],\n",
      "        [5.],\n",
      "        [6.]])\n",
      "torch.Size([1599, 11])\n",
      "torch.Size([1599, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_tensor,categories = read_wine_data()\n",
    "#results = list(map(int, results))\n",
    "print(data_tensor.shape)\n",
    "print(categories)\n",
    "inputs = data_tensor[:,:-1]\n",
    "targets = data_tensor[:,-1].unsqueeze(-1)\n",
    "print(targets)\n",
    "print(inputs.shape)\n",
    "print(targets.shape)\n",
    "#results = list(map(int, results))\n",
    "\n",
    "lr_model = nn.Linear(in_features=11,out_features=1)\n",
    "optimizer = torch.optim.SGD(lr_model.parameters(),lr=0.0001)\n",
    "criterion = nn.MSELoss(size_average=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, we'll create an iterable dataset in order to train our model. We'll use `TensorDataset` and  `DataLoader`\n",
    "from PyTorch. `TensorDataset` takes inputs and targets tensors as arguments and wraps them together. Then `Dataloader`\n",
    "combines a dataset and a sampler,  provides an iterable over the given dataset and generates batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# Define dataset\n",
    "train_ds = TensorDataset(inputs, targets)\n",
    "# Define data loader\n",
    "batch_size = 100\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "#next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Average_loss 42.85\n",
      "Epoch 1 Average_loss 6.92\n",
      "Epoch 2 Average_loss 4.20\n",
      "Epoch 3 Average_loss 2.73\n",
      "Epoch 4 Average_loss 1.89\n",
      "Epoch 5 Average_loss 1.40\n",
      "Epoch 6 Average_loss 1.13\n",
      "Epoch 7 Average_loss 0.97\n",
      "Epoch 8 Average_loss 0.88\n",
      "Epoch 9 Average_loss 0.82\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for i in range(epochs):\n",
    "    average_loss = 0.0\n",
    "    for batch_index, (x,y) in enumerate(train_dl):\n",
    "        preds = lr_model(x)\n",
    "        loss = criterion(preds,y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        average_loss +=loss.item()\n",
    "    print(f'Epoch {i} Average_loss {average_loss/len(train_dl):.2f}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "1.7.linear_regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}