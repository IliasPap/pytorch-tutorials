{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fad577a3d68>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "torch.__version__\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Linear Regression\n",
    "\n",
    "useful links\n",
    "\n",
    "https://www.youtube.com/watch?v=zPG4NjIkCjc\n",
    "\n",
    "https://www.kaggle.com/aakashns/pytorch-basics-linear-regression-from-scratch\n",
    "\n",
    "Linear regression models a linear relationship between two variables. There is usually an independent value $x$\n",
    "and a dependent value $y$. Linear regression has an equation with the form $y=ax+b$ and finds the optimal values\n",
    "$a$ and $b$ that best describe the relationship of the variables. More specifically, this equation describes a straight\n",
    "line with slope eaual to $a$ and $b$ the intercept (the value of $y$ when $x = 0$).\n",
    "\n",
    "\n",
    "\n",
    "Let's create and initialize randomly our model's variables  $a$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1919], requires_grad=True)\n",
      "tensor([1.2638], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a = torch.randn(1,requires_grad=True)\n",
    "b = torch.randn(1,requires_grad=True)\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "\n",
    "x = torch.randn(10)\n",
    "y = a*x+b\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Fit our model to random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8456],\n",
      "        [-0.6027],\n",
      "        [-1.4105],\n",
      "        [-0.1005],\n",
      "        [-0.4007],\n",
      "        [-0.5887],\n",
      "        [-0.6124],\n",
      "        [-0.8205],\n",
      "        [-0.3867],\n",
      "        [-0.2400]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = torch.randn((10,1))\n",
    "x = x*2\n",
    "y = torch.randn((10,1))\n",
    "y * y*2\n",
    "\n",
    "\n",
    "class Linear_regression_model:\n",
    "    def __init__(self):\n",
    "        super(Linear_regression_model, self).__init__()\n",
    "\n",
    "        self.a = torch.randn((1,1),requires_grad=True)\n",
    "        self.b = torch.randn((1,1),requires_grad=True)\n",
    "        self.lr = 0.001\n",
    "    def forward(self,x):\n",
    "        y_hat = torch.matmul(x , self.a.t()) + self.b\n",
    "        #y_hat = x@  self.a.t() + self.b\n",
    "        return y_hat\n",
    "    def mse_loss(self,y,y_hat):\n",
    "        diff = y-y_hat\n",
    "        self.loss = (diff*diff).mean()\n",
    "        \n",
    "        return self.loss.item()\n",
    "    def update_params(self):\n",
    "        self.loss.backward()\n",
    "        grad_a = - self.lr*self.a.grad\n",
    "        grad_b = - self.lr*self.b.grad\n",
    "        self.a = self.a +grad_a\n",
    "        self.b = self.b +grad_b\n",
    "        if self.a.grad is not None:\n",
    "            self.a.grad.zero_()\n",
    "        if self.b.grad is not None:\n",
    "            self.b.grad.zero_()\n",
    "\n",
    "model = Linear_regression_model()\n",
    "y_hat = model.forward(x)\n",
    "\n",
    "print(y_hat)\n",
    "# y_hat.mean().backward()\n",
    "# print(model.a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "MSE LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def mse(x1,x2):\n",
    "#     return ((x1-x2)**2).mean()\n",
    "#\n",
    "# loss = mse(y,y_hat)\n",
    "# print(loss)\n",
    "#\n",
    "# loss.backward()\n",
    "\n",
    "loss = model.mse_loss(y,y_hat)\n",
    "lr = 0.0001\n",
    "# print(f'Before update {a}  {b} Gradients {a.grad} {b.grad}')\n",
    "# a = a-lr*a.grad\n",
    "# b = b-lr*b.grad\n",
    "# if a.grad is not None:\n",
    "#     a.grad.zero_()\n",
    "# if b.grad is not None:\n",
    "#     b.grad.zero_()\n",
    "#\n",
    "#\n",
    "#\n",
    "# print(f'After update {a}  {b}')\n",
    "# def update_parameters(a,b,lr=0.0001):\n",
    "#     a = a-lr*a.grad\n",
    "#     b = b-lr*b.grad\n",
    "#     b.grad.zero_()\n",
    "#     a.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "MSE LOSS\n",
    "\n",
    "$MSE = \\frac{1}{N}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights and biases\n",
    "a = torch.randn(2, 3, requires_grad=True)\n",
    "b = torch.randn(2, requires_grad=True)\n",
    "\n",
    "a = torch.randn(1, 1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "print(a)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the model\n",
    "def model(x):\n",
    "    return x @ a.t() + b\n",
    "\n",
    "# MSE loss\n",
    "def mse(t1, t2):\n",
    "    diff = t1 - t2\n",
    "    return torch.sum(diff * diff) / diff.numel()\n",
    "def mse(y,y_hat):\n",
    "     return((y-y_hat)**2).mean()\n",
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]], dtype='float32')\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119]], dtype='float32')\n",
    "\n",
    "\n",
    "# Input (temp, rainfall, humidity)\n",
    "inputs = 0.001*np.array([[73 ], \n",
    "                   [91 ], \n",
    "                   [87], \n",
    "                   [102], \n",
    "                   [69]], dtype='float32')\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "targets = 0.001*np.array([[56], \n",
    "                    [81], \n",
    "                    [119], \n",
    "                    [22], \n",
    "                    [103]], dtype='float32')\n",
    "\n",
    "# Convert inputs and targets to tensors\n",
    "\n",
    "print(inputs.shape)\n",
    "print(targets.shape)\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "print(preds)\n",
    "\n",
    "\n",
    "# Compute loss\n",
    "loss = mse(preds, targets)\n",
    "print(loss)\n",
    "\n",
    "# Compute gradients\n",
    "loss.backward()\n",
    "\n",
    "\n",
    "\n",
    "# Gradients for weights\n",
    "print(a)\n",
    "print(a.grad)\n",
    "\n",
    "\n",
    "\n",
    "# Gradients for bias\n",
    "print(b)\n",
    "print(b.grad)\n",
    "\n",
    "\n",
    "\n",
    "a.grad.zero_()\n",
    "b.grad.zero_()\n",
    "print(a.grad)\n",
    "print(b.grad)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train for 100 epochs\n",
    "lr = 1e-3\n",
    "for i in range(100):\n",
    "    preds = model(inputs)\n",
    "    loss = mse(preds, targets)\n",
    "    print(f'Loss {loss.item():.2f}')\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        a -= a.grad * lr\n",
    "        b -= b.grad * lr\n",
    "        a.grad.zero_()\n",
    "        b.grad.zero_()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# model = Linear_regression_model()\n",
    "# num_of_iterations = 100\n",
    "# for i in range(num_of_iterations):\n",
    "#     y_hat =model.forward(x)\n",
    "#     loss = model.mse_loss(y,y_hat)\n",
    "#     print(f'LOSS {loss}')\n",
    "#     model.update_params()\n",
    "\n",
    "\n",
    "x1 = torch.randn(10,1)\n",
    "\n",
    "x = torch.tensor(x1)\n",
    "\n",
    "y = torch.randn(10,1)\n",
    "\n",
    "\n",
    "\n",
    "a = torch.randn(1,1,requires_grad=True)\n",
    "b = torch.randn(1,requires_grad=True)\n",
    "num_of_iterations = 100\n",
    "for i in range(num_of_iterations):\n",
    "    y_hat = x@a.t() + b\n",
    "    loss = ((y-y_hat)**2).mean()\n",
    "    print(f'LOSS {loss.item()}')\n",
    "    loss.backward()\n",
    "\n",
    "    a = a-lr*a.grad\n",
    "    b = b-lr*b.grad\n",
    "    if a.grad is not None:\n",
    "        a.grad.zero_()\n",
    "    if b.grad is not None:\n",
    "        b.grad.zero_()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3698]], requires_grad=True)\n",
      "tensor([-1.6349], requires_grad=True)\n",
      "(5, 1)\n",
      "(5, 1)\n",
      "tensor([[-1.6619],\n",
      "        [-1.6686],\n",
      "        [-1.6671],\n",
      "        [-1.6726],\n",
      "        [-1.6604]], grad_fn=<AddBackward0>)\n",
      "tensor(3.0367, grad_fn=<MeanBackward0>)\n",
      "tensor([[-0.3698]], requires_grad=True)\n",
      "tensor([[-0.2938]])\n",
      "tensor([-1.6349], requires_grad=True)\n",
      "tensor([-3.4846])\n",
      "tensor([[0.]])\n",
      "tensor([0.])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 3.04\n",
      "Loss 3.02\n",
      "Loss 3.01\n",
      "Loss 3.00\n",
      "Loss 2.99\n",
      "Loss 2.98\n",
      "Loss 2.96\n",
      "Loss 2.95\n",
      "Loss 2.94\n",
      "Loss 2.93\n",
      "Loss 2.92\n",
      "Loss 2.90\n",
      "Loss 2.89\n",
      "Loss 2.88\n",
      "Loss 2.87\n",
      "Loss 2.86\n",
      "Loss 2.85\n",
      "Loss 2.84\n",
      "Loss 2.82\n",
      "Loss 2.81\n",
      "Loss 2.80\n",
      "Loss 2.79\n",
      "Loss 2.78\n",
      "Loss 2.77\n",
      "Loss 2.76\n",
      "Loss 2.75\n",
      "Loss 2.73\n",
      "Loss 2.72\n",
      "Loss 2.71\n",
      "Loss 2.70\n",
      "Loss 2.69\n",
      "Loss 2.68\n",
      "Loss 2.67\n",
      "Loss 2.66\n",
      "Loss 2.65\n",
      "Loss 2.64\n",
      "Loss 2.63\n",
      "Loss 2.62\n",
      "Loss 2.61\n",
      "Loss 2.59\n",
      "Loss 2.58\n",
      "Loss 2.57\n",
      "Loss 2.56\n",
      "Loss 2.55\n",
      "Loss 2.54\n",
      "Loss 2.53\n",
      "Loss 2.52\n",
      "Loss 2.51\n",
      "Loss 2.50\n",
      "Loss 2.49\n",
      "Loss 2.48\n",
      "Loss 2.47\n",
      "Loss 2.46\n",
      "Loss 2.45\n",
      "Loss 2.44\n",
      "Loss 2.43\n",
      "Loss 2.42\n",
      "Loss 2.41\n",
      "Loss 2.40\n",
      "Loss 2.39\n",
      "Loss 2.38\n",
      "Loss 2.37\n",
      "Loss 2.37\n",
      "Loss 2.36\n",
      "Loss 2.35\n",
      "Loss 2.34\n",
      "Loss 2.33\n",
      "Loss 2.32\n",
      "Loss 2.31\n",
      "Loss 2.30\n",
      "Loss 2.29\n",
      "Loss 2.28\n",
      "Loss 2.27\n",
      "Loss 2.26\n",
      "Loss 2.25\n",
      "Loss 2.24\n",
      "Loss 2.24\n",
      "Loss 2.23\n",
      "Loss 2.22\n",
      "Loss 2.21\n",
      "Loss 2.20\n",
      "Loss 2.19\n",
      "Loss 2.18\n",
      "Loss 2.17\n",
      "Loss 2.16\n",
      "Loss 2.16\n",
      "Loss 2.15\n",
      "Loss 2.14\n",
      "Loss 2.13\n",
      "Loss 2.12\n",
      "Loss 2.11\n",
      "Loss 2.10\n",
      "Loss 2.10\n",
      "Loss 2.09\n",
      "Loss 2.08\n",
      "Loss 2.07\n",
      "Loss 2.06\n",
      "Loss 2.05\n",
      "Loss 2.05\n",
      "Loss 2.04\n"
     ]
    }
   ],
   "source": [
    "# Train for 100 epochs\n",
    "lr = 1e-3\n",
    "for i in range(100):\n",
    "    preds = model(inputs)\n",
    "    loss = mse(preds, targets)\n",
    "    print(f'Loss {loss.item():.2f}')\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        a -= a.grad * lr\n",
    "        b -= b.grad * lr\n",
    "        a.grad.zero_()\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS 1.2500154972076416\n",
      "LOSS 1.247922658920288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iliasprc/Documents/penvs/venv/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if sys.path[0] == '':\n",
      "/home/iliasprc/Documents/penvs/venv/lib/python3.6/site-packages/ipykernel_launcher.py:29: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "/home/iliasprc/Documents/penvs/venv/lib/python3.6/site-packages/ipykernel_launcher.py:31: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "/home/iliasprc/Documents/penvs/venv/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-39-33afce62afc9>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     25\u001B[0m     \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 27\u001B[0;31m     \u001B[0ma\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0ma\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0mlr\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgrad\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     28\u001B[0m     \u001B[0mb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mb\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0mlr\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mb\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgrad\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     29\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0ma\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgrad\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "\n",
    "# model = Linear_regression_model()\n",
    "# num_of_iterations = 100\n",
    "# for i in range(num_of_iterations):\n",
    "#     y_hat =model.forward(x)\n",
    "#     loss = model.mse_loss(y,y_hat)\n",
    "#     print(f'LOSS {loss}')\n",
    "#     model.update_params()\n",
    "\n",
    "\n",
    "x1 = torch.randn(10,1)\n",
    "\n",
    "x = torch.tensor(x1)\n",
    "\n",
    "y = torch.randn(10,1)\n",
    "\n",
    "\n",
    "\n",
    "a = torch.randn(1,1,requires_grad=True)\n",
    "b = torch.randn(1,requires_grad=True)\n",
    "num_of_iterations = 100\n",
    "for i in range(num_of_iterations):\n",
    "    y_hat = x@a.t() + b\n",
    "    loss = ((y-y_hat)**2).mean()\n",
    "    print(f'LOSS {loss.item()}')\n",
    "    loss.backward()\n",
    "\n",
    "    a = a-lr*a.grad\n",
    "    b = b-lr*b.grad\n",
    "    if a.grad is not None:\n",
    "        a.grad.zero_()\n",
    "    if b.grad is not None:\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}