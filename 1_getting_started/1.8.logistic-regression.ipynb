{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'1.6.0'"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Logisticr](./figures/1.8.logistic.png)\n",
    "\n",
    "import matplotlib.pyplot as pyplot\n",
    "x =10.*(torch.rand(500)-0.5).numpy()\n",
    "y = (torch.rand(500)).numpy()\n",
    "\n",
    "line_x = 5 * torch.arange(-500,500).float()/500.0\n",
    "sigmoid_y = torch.sigmoid(line_x)\n",
    "\n",
    "#print(x[x > 0.5])\n",
    "pyplot.xlabel('X')\n",
    "pyplot.ylabel('Y')\n",
    "pyplot.plot(x, y, 'bo')\n",
    "pyplot.plot(line_x , sigmoid_y,'r',linewidth=4, markersize=12)\n",
    "pyplot.show()\n",
    "\n",
    "\n",
    "\n",
    "# 1.8 Introduction to logistic regression\n",
    "In this chapter, we will deal with structured data and use logistic regression to classify structured data simply.\n",
    "Logistic regression is a statistical model that adopts a logistic function to model a binary dependent\n",
    "variable, although many more complex extensions exist. In regression analysis, logistic regression\n",
    "is estimating the parameters of a logistic model in the form of binary regression.\n",
    "Mathematically, a binary logistic model has a dependent variable with two possible values,\n",
    "such as \"True/False\" or \"Yes/No\" which is represented by an indicator variable $p$ that is labeled with \"0\" or \"1\".\n",
    "Logistic regression is a kind of generalized linear regression (generalized linear model), which has many similarities\n",
    "with multiple linear regression analysis. Their model formulas are basically the same, both have $wx + b$, where $w$ and\n",
    "$b$ are the parameters to be learned. The major difference lies in their different dependent variables, multiple linear\n",
    "regression directly uses $wx+b$ as the dependent variable, that is, $y = wx+b$. However, logistic regression uses the\n",
    "function $L$ to correspond $wx+b$ to a hidden state $p$, $p = L(wx+b)$, and then determine the value of the\n",
    "dependent variable according\n",
    "to the size of $p$ and $1-p$. If $L$ is a logistic function, it is logistic regression while if $L$ is a polynomial\n",
    "function,\n",
    "it is polynomial regression.\n",
    "In general, logistic regression will add a layer of a non-linear function on top of a linear regression layer.\n",
    "Logistic regression is mainly for two-class prediction. We talked about the Sigmoid function in the activation function.\n",
    "The Sigmoid function is the most common logistic function, because the output of the Sigmoid function is the probability\n",
    "value between 0 and 1, when the probability is greater than 0.5 is predicted as 1, and less than 0.5 is predicted as 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8.2 UCI German Credit Data Set\n",
    "\n",
    "UCI German Credit is UCI's German credit data set, which contains original data and numerical data.\n",
    "The German Credit data is a data set that predicts the risk on loans based on personal\n",
    "information and overdue loan applications from customers. The data set contains 1000 pieces of data in 24 dimensions.\n",
    "This dataset classifies people described by a set of attributes as good or bad credit risks.\n",
    "Comes in two formats (one all numeric).\n",
    "Here we directly use the processed numerical data as a display.\n",
    "\n",
    "[Address](https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and normalization\n",
    "Now we can load the data and we arte going to normalize each attribute by calculating\n",
    "the mean and standard deviation of each feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1])\n",
      "torch.Size([1000, 24])\n"
     ]
    }
   ],
   "source": [
    "data=np.loadtxt(\"./data/german.data-numeric\")\n",
    "targets = torch.from_numpy(data)[:,-1][:,None].float()-1.\n",
    "data_tensor = torch.from_numpy(data)[:,:-1]\n",
    "means = torch.mean(data_tensor,dim=0)\n",
    "stds = torch.std(data_tensor,dim=0)\n",
    "data_tensor = ((data_tensor-means)/stds).float()\n",
    "print(targets.shape)\n",
    "print(data_tensor.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distinguish between the training set and the test set. Since there is no official verification set here, we can\n",
    "directly use the accuracy of the test set as the criterion to classify  a sample as  good or bad.\n",
    "\n",
    "Split dataset: 900 for training and 100 for testing\n",
    "\n",
    "The format of the dataset is that the first 24 columns are the attributes while the last one is the label\n",
    "(1 or 2).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = data_tensor[:900,:]\n",
    "train_targets = targets[:900,:]\n",
    "test_data = data_tensor[900:,:]\n",
    "test_targets = targets[900:,:]\n",
    "batch_size = 32\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "training_dataset = TensorDataset(train_data,train_targets)\n",
    "train_loader = DataLoader(training_dataset, batch_size, shuffle=True)\n",
    "test_dataset = TensorDataset(test_data,test_targets)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle=True)\n",
    "#print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define the model, the model is very simple, a Linear layer (`nn.Linear()`)\n",
    " with a Sigmoid activation function (`nn.Sigmoid()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegressionModel,self).__init__()\n",
    "        self.model = nn.Linear(24,1) # Since 24 dimensions have been fixed, write 24 here\n",
    "        self.L_function = nn.Sigmoid()\n",
    "    def forward(self,x):\n",
    "        y = self.model(x)\n",
    "        p = self.L_function(y)\n",
    "        return p\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our model, optimizer and loss functions. It is a binary classification problem so we'll use `nn.BCELoss`\n",
    "$BCELoss(y,\\hat{y}) = -\\frac{1}{N}\\sum_{i=0}^{N}(y*\\log{\\hat{y}}+(1-y)*\\log{(1-\\hat{y})})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LogisticRegressionModel()\n",
    "criterion = nn.BCELoss(size_average=True)# Use CrossEntropyLoss loss\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=0.001) # Adam optimization\n",
    "epochs = 1000 # Train 1000 times\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0 Loss 0.76 Batch = 28 0.53\n",
      "Test Epoch 0 Loss 0.77 0.47 \n",
      "Training Epoch 1 Loss 0.73 Batch = 28 0.57\n",
      "Test Epoch 1 Loss 0.78 0.49 \n",
      "Training Epoch 2 Loss 0.70 Batch = 28 0.60\n",
      "Test Epoch 2 Loss 0.75 0.54 \n",
      "Training Epoch 3 Loss 0.68 Batch = 28 0.63\n",
      "Test Epoch 3 Loss 0.70 0.56 \n",
      "Training Epoch 4 Loss 0.66 Batch = 28 0.67\n",
      "Test Epoch 4 Loss 0.69 0.56 \n",
      "Training Epoch 5 Loss 0.64 Batch = 28 0.69\n",
      "Test Epoch 5 Loss 0.63 0.60 \n",
      "Training Epoch 6 Loss 0.63 Batch = 28 0.71\n",
      "Test Epoch 6 Loss 0.64 0.65 \n",
      "Training Epoch 7 Loss 0.62 Batch = 28 0.73\n",
      "Test Epoch 7 Loss 0.64 0.67 \n",
      "Training Epoch 8 Loss 0.60 Batch = 28 0.74\n",
      "Test Epoch 8 Loss 0.58 0.71 \n",
      "Training Epoch 9 Loss 0.60 Batch = 28 0.75\n",
      "Test Epoch 9 Loss 0.63 0.73 \n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    # Specify the model as training mode and calculate the gradient\n",
    "    net.train()\n",
    "    # Input values need to be converted into torch Tensor\n",
    "    avg_loss = 0.\n",
    "    total_samples =0.\n",
    "    correct_predictions = 0.\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "         #Clear the loss of the previous step\n",
    "        optimizer.zero_grad()\n",
    "        y_hat=net(data)\n",
    "        loss=criterion(y_hat,target) # calculate loss\n",
    "        avg_loss+=loss.item()\n",
    "        pred_label = (y_hat>0.5).int()\n",
    "        #print(pred_label)\n",
    "        total_samples += data.size()[0]\n",
    "        correct_predictions += (pred_label == target).sum()\n",
    "        loss.backward() # Backpropagation\n",
    "        optimizer.step() # optimization\n",
    "        #if (batch_idx+1) % 100 == 0: # Here we output relevant information every 100 times\n",
    "    print(f'Training Epoch {i} Loss {avg_loss/batch_idx:.2f} Batch = {batch_idx} {correct_predictions/total_samples:.2f}')\n",
    "        # Specify the model as calculation mode\n",
    "    net.eval()\n",
    "    avg_loss = 0.\n",
    "    total_samples =0.\n",
    "    correct_predictions = 0.\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            y_hat=net(data)\n",
    "            loss=criterion(y_hat,target) # calculate loss\n",
    "            avg_loss+=loss.item()\n",
    "            pred_label = (y_hat>0.5).int()\n",
    "            total_samples += data.size()[0]\n",
    "            correct_predictions += (pred_label == target).sum()\n",
    "        print(f'Test Epoch {i} Loss {avg_loss/len(test_loader):.2f} {(correct_predictions/total_samples):.2f} ')\n",
    "        # test_in = torch.from_numpy(test_data).float()\n",
    "        # test_l = torch.from_numpy(test_lab).long()\n",
    "        # test_out = net(test_in)\n",
    "        # # Use our test function to calculate accuracy\n",
    "        # accuracy = test(test_out,test_l)\n",
    "        # print(\"Epoch:{},Loss:{:.4f},Accuracy:{:.2f}\".format(i+1,loss.item(),accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training is complete, our accuracy reached ~75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}