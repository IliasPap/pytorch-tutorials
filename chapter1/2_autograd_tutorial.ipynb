{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Pytorch for Deeplearning",
      "language": "python",
      "name": "pytorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "2_autograd_tutorial.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJmmcSq-Dkim",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLB20JTHDkiu",
        "colab_type": "text"
      },
      "source": [
        "Autograd: automatic derivation mechanism\n",
        "===================================\n",
        "\n",
        "The core of all neural networks in PyTorch is the `` autograd`` package.\n",
        "Let's briefly introduce this package, and then train the first simple neural network.\n",
        "\n",
        "The `` autograd '' package provides automatic derivation for all operations on tensors.\n",
        "It is a framework defined at runtime, which means that backpropagation is based on your code to determine how to run, and each iteration can be different.\n",
        "\n",
        "\n",
        "Examples\n",
        "\n",
        "Tensor\n",
        "--------\n",
        "\n",
        "`` torch.Tensor '' is the core class of this package. If set\n",
        "`` .requires_grad '' is `` True``, then all operations on the tensor will be tracked.\n",
        "When the calculation is completed, all gradients are calculated automatically by calling `` .backward () ``,\n",
        "All gradients of this tensor will automatically accumulate in the `` .grad`` attribute.\n",
        "\n",
        "To prevent the tensor from tracking the history, you can call the `` .detach () '' method to separate it from the calculation history and prohibit tracking of future calculation records.\n",
        "\n",
        "To prevent tracking of history (and use of memory), you can wrap the code block in `` with torch.no_grad (): ''.\n",
        "It is particularly useful when evaluating the model, because the model may have trainable parameters of `requires_grad = True`, but we do not need gradient calculations.\n",
        "\n",
        "There is another important class `` Function '' in automatic gradient calculation.\n",
        "\n",
        "`` Tensor`` and `` Function`` are interconnected and build up an acyclic\n",
        "graph, that encodes a complete history of computation. Each tensor has\n",
        "a `` .grad_fn`` attribute that references a `` Function`` that has created\n",
        "the `` Tensor`` (except for Tensors created by the user-their\n",
        "`` grad_fn is None``).\n",
        "\n",
        "`` Tensor`` and `` Function`` are connected to each other and generate an acyclic graph, which represents and stores the complete calculation history.\n",
        "Each tensor has a `` .grad_fn '' attribute, which refers to a `` Function '' that created `` Tensor '' (unless this tensor was manually created by the user, that is, this tensor\n",
        "`` grad_fn`` is `` None``).\n",
        "\n",
        "If you need to calculate the derivative, you can call .backward () on Tensor.\n",
        "If `` Tensor '' is a scalar (that is, it contains an element data), there is no need to specify any parameters for `` backward () '',\n",
        "But if it has more elements, you need to specify a `` gradient '' parameter to match the shape of the tensor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KOXS1ALDkiw",
        "colab_type": "text"
      },
      "source": [
        "*** Translator's Note: In other articles you may see that wrapping Tensor into Variable provides automatic gradient calculation. Variable has been marked as expired in version 0.41, and you can use Tensor directly now it's here:***\n",
        "(https://pytorch.org/docs/stable/autograd.html#variable-deprecated)\n",
        "\n",
        "There will be a detailed description later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFucQN0NDkix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkDovu48Dki3",
        "colab_type": "text"
      },
      "source": [
        "Create a tensor and set requires_grad = True to track his calculation history\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c375zoI9Dki4",
        "colab_type": "code",
        "colab": {},
        "outputId": "1f5354d7-277e-47f6-f545-f5d68d9ac4e9"
      },
      "source": [
        "x = torch.ones(2, 2, requires_grad=True)\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BziFnoZxDki-",
        "colab_type": "text"
      },
      "source": [
        "Operate on tensors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62JFPFDDDki_",
        "colab_type": "code",
        "colab": {},
        "outputId": "7853c58e-2cf3-41b0-e238-d3239352d29a"
      },
      "source": [
        "y = x + 2\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[3., 3.],\n",
            "        [3., 3.]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCaewTxBDkjD",
        "colab_type": "text"
      },
      "source": [
        "The result `` y `` has been calculated, so, `` grad_fn `` has been automatically generated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0itFUHVoDkjE",
        "colab_type": "code",
        "colab": {},
        "outputId": "c65a0aca-1eeb-4e04-9499-3ed048c1b1ca"
      },
      "source": [
        "print(y.grad_fn)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<AddBackward0 object at 0x000002004F7CC248>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq55cslCDkjI",
        "colab_type": "text"
      },
      "source": [
        "Perform an operation on y\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DVuxDAODkjI",
        "colab_type": "code",
        "colab": {},
        "outputId": "02791957-86af-4e1c-910f-841a58478358"
      },
      "source": [
        "z = y * y * 3\n",
        "out = z.mean()\n",
        "\n",
        "print(z, out)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[27., 27.],\n",
            "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNZ8YHAqDkjM",
        "colab_type": "text"
      },
      "source": [
        "`` .requires_grad_ `` (...)  can change the `` requires_grad`` attribute of an existing tensor.\n",
        "\n",
        "\n",
        "If not specified, the default input flag is `` False``.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQVhylt7DkjM",
        "colab_type": "code",
        "colab": {},
        "outputId": "ae9655dd-2db1-4cda-bd1f-5730eaf662d4"
      },
      "source": [
        "a = torch.randn(2, 2)\n",
        "a = ((a * 3) / (a - 1))\n",
        "print(a.requires_grad)\n",
        "a.requires_grad_(True)\n",
        "print(a.requires_grad)\n",
        "b = (a * a).sum()\n",
        "print(b.grad_fn)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "True\n",
            "<SumBackward0 object at 0x000002004F7D5608>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yrYVBHxDkjP",
        "colab_type": "text"
      },
      "source": [
        "gradient\n",
        "---------\n",
        "Back propagation\n",
        "Because `` out `` is a scalar, `` out.backward () `` is equal to `` out.backward (torch.tensor (1)) ``.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmHvCCmhDkjQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out.backward()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHfnZP3bDkjS",
        "colab_type": "text"
      },
      "source": [
        "print gradients d(out)/dx\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zper_SiNDkjT",
        "colab_type": "code",
        "colab": {},
        "outputId": "a02fd811-2c8b-45e5-b15c-beb4374e1f11"
      },
      "source": [
        "print(x.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[4.5000, 4.5000],\n",
            "        [4.5000, 4.5000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeXwy5jgDkjW",
        "colab_type": "text"
      },
      "source": [
        "Get the matrix `` 4.5 ``. Call `` out ``\n",
        "*Tensor* “$o$”.\n",
        "\n",
        "$o = \\frac{1}{4}\\sum_i z_i$,\n",
        "$z_i = 3(x_i+2)^2$ $z_i\\bigr\\rvert_{x_i=1} = 27$.\n",
        "\n",
        ",\n",
        "$\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)$, \n",
        "$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S29L9r1mDkjX",
        "colab_type": "text"
      },
      "source": [
        "Mathematically, if we have a vector-valued function $\\vec{y} = f(\\vec{x}))$ ， the gradient of $\\vec{y}$ with respect to $\\vec{x}$ is a Jacobian Matrix (Jacobian matrix)：\n",
        "\n",
        "$J = \\begin{pmatrix} \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}} \\end{pmatrix}$\n",
        "\n",
        "Generally speaking, `torch.autograd` is a tool for calculating vector-Jacobian product. That is, given any vector $v=(v_{1}\\;v_{2}\\;\\cdots\\;v_{m})^{T}$ ，calculate $v^{T}\\cdot J$ ， if $v$ happens to be the gradient of the scalar function $l=g(\\vec{y})$ , which means  $v=(\\frac{\\partial l}{\\partial  y_{1}}\\;\\cdots\\;\\frac{\\partial l}{\\partial  y_{m}})^{T}$，then according to the chain rule, the vector-Jacobian product is the gradient of $\\vec{x}$：\n",
        "\n",
        "$J^{T}\\cdot v = \\begin{pmatrix} \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}} \\end{pmatrix} \\begin{pmatrix} \\frac{\\partial l}{\\partial y_{1}}\\\\ \\vdots \\\\ \\frac{\\partial l}{\\partial y_{m}} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial l}{\\partial x_{1}}\\\\ \\vdots \\\\ \\frac{\\partial l}{\\partial x_{n}} \\end{pmatrix}$\n",
        "\n",
        "(Note that $v^{T}\\cdot J$ gives a row vector, which can be treated as a column vector by $ J ^ {T} \\cdot v $)\n",
        "\n",
        "This feature makes it very convenient to return external gradients to models with nonscalar outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HG2gPIGNDkjX",
        "colab_type": "text"
      },
      "source": [
        "Now let's look at an example of vector-Jacobian product"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPsyjjUkDkjY",
        "colab_type": "code",
        "colab": {},
        "outputId": "7cc885bb-b1a8-453a-b134-95f100a1a385"
      },
      "source": [
        "x = torch.randn(3, requires_grad=True)\n",
        "\n",
        "y = x * 2\n",
        "while y.data.norm() < 1000:\n",
        "    y = y * 2\n",
        "\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 293.4463,   50.6356, 1031.2501], grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dI37g62Dkja",
        "colab_type": "text"
      },
      "source": [
        "In this case, `y` is no longer a scalar. `torch.autograd` cannot directly calculate the complete Jacobian ranks, but if we only want the vector-Jacobian product, we just pass the vector as a parameter to` backward`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-_byEaRDkjb",
        "colab_type": "code",
        "colab": {},
        "outputId": "44e109fb-f28b-4ae9-e763-0deeb702b1df"
      },
      "source": [
        "gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
        "y.backward(gradients)\n",
        "\n",
        "print(x.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIEHTLPUDkjd",
        "colab_type": "text"
      },
      "source": [
        "If `` .requires_grad = True `` but you do n’t want autograd calculation\n",
        "Then you can wrap the variable in `` with torch.no_grad () ``:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZQ76117Dkje",
        "colab_type": "code",
        "colab": {},
        "outputId": "2936c284-9544-4746-f739-9219b43a7236"
      },
      "source": [
        "print(x.requires_grad)\n",
        "print((x ** 2).requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "\tprint((x ** 2).requires_grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rG7hjCLsDkjg",
        "colab_type": "text"
      },
      "source": [
        "** Read later: **\n",
        "\n",
        "  Official documentation for `` autograd`` and `` Function`` https://pytorch.org/docs/autograd\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePL9EJx6Dkjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}