{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. PyTorch AI Summer\n",
    "\n",
    "In this article, we will give a brief introduction about PyTorch framework and we'll discuss the 2 main features : `Torch.Tensor` and `autograd`.\n",
    "\n",
    "# 1.1 Introduction to Pytorch\n",
    "\n",
    "\n",
    "PyTorch is an open-source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing.\n",
    "Torch is an open-source machine learning library, a scientific computing framework, and a scripting language based on the Lua programming language. It provides a wide range of algorithms for deep learning, and uses the scripting language LuaJIT, and an underlying C implementation. As of 2018, Torch is no longer in active development. PyTorch is primarily developed by Facebook's AI Research lab (FAIR). It is free and open-source software released under the Modified BSD license. Although the Python interface is more polished and the primary focus of development, PyTorch also has a C++ interface.\n",
    "\n",
    "Many pieces of Deep Learning applications are built on top of PyTorch, including Uber's Pyro, HuggingFace's Transformers, and Catalyst.\n",
    "\n",
    "PyTorch provides two high-level features:\n",
    "\n",
    "  - Tensor computing (like NumPy) with strong acceleration via graphics processing units (GPU)\n",
    "  - Deep neural networks building with an automatic differential system\n",
    "\n",
    "\n",
    "PyTorch and Torch use C libraries containing all the same performance: TH, THC, THNN, THCUNN, and they will continue to share these libraries.\n",
    "\n",
    "This answer is very clear, in fact, PyTorch and Torch both use the same bottom layer, but have different upper packaging languages.\n",
    "You can reuse your favourite Python packages such as NumPy, SciPy and Cython to extend PyTorch when needed.\n",
    "\n",
    "- [GitHub](https://github.com/pytorch/pytorch)\n",
    "- [WebPage](https://pytorch.org/)\n",
    "\n",
    "<!---\n",
    "## 1.1.1 Compare PyTorch and Tensorflow\n",
    "There is no such thing as what's the best framework, but only which is more suitable. [This article](https://zhuanlan.zhihu.com/p/28636490) has a simple comparison, so I won’t go into details here.\n",
    "And the technology is developed, and the comparison is not absolute.\n",
    "For example, Tensorflow introduced the Eager Execution mechanism to implement dynamic graphs in version 1.5, PyTorch visualization, windows support, and tensor flips along the dimension have all been issues. Not a problem.\n",
    "-->\n",
    "\n",
    "- PyTorch is a very simple, elegant, efficient and fast framework\n",
    "- The design pursues the least package, and try to avoid re-creating the wheels\n",
    "- It has the most elegant object-oriented design in all frameworks, and the design is most in line with people's thinking. It allows users to focus on implementing their own ideas as much as possible.\n",
    "- Big  support, similar to Google ’s Tensorflow, FAIR support is enough to ensure PyTorch gets continuous development updates\n",
    "- Good documentation (compared to other FB projects, PyTorch's documentation is almost perfect), a forum personally maintained by PyTorch authors for users to communicate and ask questions\n",
    "- Easy to get started with machine / deep learning\n",
    "\n",
    "So if the above information has something that appeals to you, then be sure to finish reading this article."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PyTorch Tensor operations\n",
    "<a href=\"https://colab.research.google.com/github/iliasprc/pytorch-tutorials/blob/master/chapter1/2_autograd_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "\n",
    "\n",
    "Getting Started\n",
    "---------------\n",
    "Now we will learn how to create Tensors as well as the main functions and operations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tensors basic operations\n",
    "Tensors are very similar to NumPy’s ``ndarrays``, except that they can also be used on a GPU device to accelerate computing.\n",
    "In the following figure, you can see a visualization of Tensors with different dimensions\n",
    "\n",
    "![tensor](images/Pytorch_book_chapter1_tensor.jpg)\n",
    "\n",
    "\n",
    "To construct a 5x3  uninitialized matrix you can use `` torch.empty(5,3)`` function:\n",
    "\n",
    "In order to find the tensor's dimensions use\n",
    "`` tensor.shape`` or ``tensor.size()`` that returns `[5,3]`(``torch.Size`` is in fact a tuple, so it supports all tuple operations)\n",
    "and  ``tensor.dtype`` to find tensor´s data type i.e. `int,float,....`\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      " Tensor shape = torch.Size([5, 3])\n",
      " Data type = torch.float32 \n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(5, 3)\n",
    "print(\"{}\\n Tensor shape = {}\\n Data type = {} \".format(x,x.shape,x.dtype))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To initialize randomly a 2D matrix:\n",
    "use ``torch.rand()``\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4214, 0.1018, 0.3309],\n",
      "        [0.6035, 0.1967, 0.5448],\n",
      "        [0.1855, 0.5156, 0.5147],\n",
      "        [0.7176, 0.1810, 0.7158],\n",
      "        [0.4004, 0.2005, 0.9869]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "\n",
    "print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can also construct a matrix filled with zeros:\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(5, 3)\n",
    "print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you have a list of numbers you can also convert it directly  to a tensor as follows:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we'll see how to create a tensor based on an existing tensor. These methods\n",
    "will reuse properties of the input tensor, e.g. `dtype`, unless\n",
    "new values are provided by user\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[ 1.0079,  1.8776, -0.3918],\n",
      "        [ 1.4096, -1.5037, -2.3139],\n",
      "        [ 0.8209, -1.0360,  0.8332],\n",
      "        [ 1.8210, -2.3557,  2.0603],\n",
      "        [-0.5113, -0.0757,  1.6113]])\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(5, 3, dtype=torch.double)      # new_* methods take in sizes\n",
    "print(x)\n",
    "x = torch.randn_like(x, dtype=torch.float)    # override dtype!\n",
    "print(x)               # result has the same size\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Operations\n",
    "With tensors, you can do all arithmetic and logical operations similar to Numpy.\n",
    "There are multiple syntaxes for those operations. In the following\n",
    "example, we will take a look at the addition operation.\n",
    "\n",
    "First syntax of matrix addition`(x+y)`\n",
    "\n",
    "![](images/chapter_1_1_tensors_addition.png)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0510,  2.4556,  0.0803],\n",
      "        [ 1.6055, -1.4762, -2.1654],\n",
      "        [ 1.7307, -0.0573,  1.5550],\n",
      "        [ 2.7760, -1.5332,  2.8199],\n",
      "        [-0.3677,  0.1496,  1.7654]])\n",
      " Tensor shape = torch.Size([5, 3])\n",
      " Data type = torch.float32 \n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(5, 3)\n",
    "c = x + y\n",
    "print(\"{}\\n Tensor shape = {}\\n Data type = {} \".format(c,c.shape,c.dtype))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "An alternative command is `torch.add(x,y)`:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0079,  1.8776, -0.3918],\n",
      "        [ 1.4096, -1.5037, -2.3139],\n",
      "        [ 0.8209, -1.0360,  0.8332],\n",
      "        [ 1.8210, -2.3557,  2.0603],\n",
      "        [-0.5113, -0.0757,  1.6113]])\n"
     ]
    }
   ],
   "source": [
    "c = torch.add(x, y)\n",
    "print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "or you can provide an output tensor as an argument:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0079,  1.8776, -0.3918],\n",
      "        [ 1.4096, -1.5037, -2.3139],\n",
      "        [ 0.8209, -1.0360,  0.8332],\n",
      "        [ 1.8210, -2.3557,  2.0603],\n",
      "        [-0.5113, -0.0757,  1.6113]])\n"
     ]
    }
   ],
   "source": [
    "c = torch.empty(5, 3)\n",
    "torch.add(x, y, out=c)\n",
    "\n",
    "print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In addition, one can do in-place operations.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0510,  2.4556,  0.0803],\n",
      "        [ 1.6055, -1.4762, -2.1654],\n",
      "        [ 1.7307, -0.0573,  1.5550],\n",
      "        [ 2.7760, -1.5332,  2.8199],\n",
      "        [-0.3677,  0.1496,  1.7654]])\n"
     ]
    }
   ],
   "source": [
    "# adds x to y\n",
    "y.add_(x)\n",
    "print(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: Any operation that mutates a tensor in-place is post-fixed with an ``_``.\n",
    "For example: ``x.copy_(y)``, ``x.t_()``, will change ``x``.\n",
    "Now, let's see how to retrieve any element of the Tensor. It's very simple. You can use standard NumPy-like indexing to access any element of the tensor.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.8776, -1.5037, -1.0360, -2.3557, -0.0757])\n",
      "tensor(1.0079)\n",
      "tensor([[ 1.0079,  1.8776],\n",
      "        [ 1.4096, -1.5037]])\n"
     ]
    }
   ],
   "source": [
    "print(x[:, 1])\n",
    "print(x[0,0])\n",
    "print(x[0:2,:-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you have an one element tensor, you can use  ``.item()`` to get the value as a\n",
    "Python number.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "0.3687810003757477"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "x.item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create tensor within a range from 0 to N using `torch.arange(N)`.\n",
    "\n",
    "![](images/chapter_1_1_tensors_range.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(10)\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create an identity matrix\n",
    "![](images/chapter_1_1_tensors_identity.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.eye(3, 3)\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "NumPy Conversion\n",
    "------------\n",
    "\n",
    "Converting a Torch Tensor to a NumPy array and vice versa is very easy.\n",
    "\n",
    "The Torch Tensor and NumPy array will share their underlying memory\n",
    "locations, and changing one will change the other.\n",
    "\n",
    "Converting a Torch Tensor to a NumPy Array\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)\n",
    "b = a.numpy()\n",
    "print(b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "See how the numpy array b changed its values after a change in a\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Converting numpy arrays to torch tensors\n",
    "\n",
    "See how changing the np array changed the torch tensor automatically\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All the tensors on the CPU except a Chartensor support converting to\n",
    "NumPy and back.\n",
    "\n",
    "CUDA Tensors\n",
    "------------\n",
    "\n",
    "Tensors can be moved onto any device using the ``.to()`` method.\n",
    "\n",
    "Let's run the following  cell to check if any CUDA-capable device is available.\n",
    "Then we  will use ``torch.device`` objects to move tensors in and out of the GPU."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 1., 1.],\n",
      "        [1., 2., 1.],\n",
      "        [1., 1., 2.]], device='cuda:0')\n",
      "tensor([[2., 1., 1.],\n",
      "        [1., 2., 1.],\n",
      "        [1., 1., 2.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to()`` can also change dtype together!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can also use ``.cpu()`` and ``.cuda()`` to transfer tensors between cpu and gpu memory\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "That was the first tutorial on Tensor's basic functions.\n",
    "In the next section, we will learn about\n",
    "PyTorch's automatic differentiation package, `autograd`.\n",
    "  For more  Tensor operations, including transposing, indexing, slicing,\n",
    "  mathematical operations, linear algebra, random numbers, etc.,\n",
    "  check the following link  `https://pytorch.org/docs/torch`.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Autograd: Automatic Differentiation\n",
    "===================================\n",
    "\n",
    "The main advantage of PyTorch framework is the ``autograd`` package.\n",
    "Let’s  briefly describe this and then we will learn how to train our\n",
    "first neural network in the following articles.\n",
    "The ``autograd`` package provides automatic differentiation for all operations\n",
    "on Tensors. It is a define-by-run framework, which means that your backpropagation is\n",
    "defined by how your code is developed and ran, as well as that every single iteration can be\n",
    "different.\n",
    "\n",
    "Let's see this in more details with some examples.\n",
    "\n",
    "Tensor\n",
    "--------\n",
    "\n",
    "``torch.Tensor`` is the main class of the package. If you set its attribute\n",
    "``.requires_grad`` as ``True``, it starts to track all tensor's operations. When\n",
    "you finish your computation you can call ``.backward()`` and have all the\n",
    "gradients computed automatically. The gradient for this tensor will be\n",
    "accumulated into ``.grad`` attribute.\n",
    "\n",
    "To stop a tensor from tracking its history, you can call ``.detach()`` to detach\n",
    "it from the computation history, and to prevent future computation from being\n",
    "tracked. To prevent tracking history (and using memory), you can also wrap the code block\n",
    "in ``with torch.no_grad():``. This can be particularly helpful when evaluating a\n",
    "model because the model may have trainable parameters with `requires_grad=True`,\n",
    "but for which we don't need the gradients. There’s one more class which is very important for autograd\n",
    "implementation - a ``Function``.\n",
    "\n",
    "``Tensor`` and ``Function`` are interconnected and build up an acyclic\n",
    "graph, that encodes a complete history of computation. Each tensor has\n",
    "a ``.grad_fn`` attribute that references a ``Function`` that has created\n",
    "the ``Tensor`` (except for Tensors created by the user - their\n",
    "``grad_fn is None``).\n",
    "\n",
    "If you want to compute the derivatives, you can call ``.backward()`` on\n",
    "a ``Tensor``. If ``Tensor`` is a scalar (i.e. it holds a one element\n",
    "data), you don’t need to specify any arguments to ``backward()``.\n",
    "However, if the tensor has more elements, you need to specify a ``gradient``\n",
    "argument that is a tensor of the same shape to your network's output.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x7f4ca9cb7930>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, create a tensor and set `requires_grad=True` to track computation history.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 1.],\n        [1., 1.]], requires_grad=True)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's do an operation with the created tensor:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[2., 2.],\n        [2., 2.]], grad_fn=<MulBackward0>)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y = 2 * x\n",
    "y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "``y`` was created as a result of a multiplication, so it has a ``grad_fn`` attribute that references to a function .\n",
    "The ``grad_fn`` will be an MulBackward0 object that confirms that operation\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "<MulBackward0 at 0x7f4c18c94828>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can do more operations on ``y`` and the tensors will still track the history\n",
    "of those operations:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(12., grad_fn=<MeanBackward0>)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = 3 * y * y\n",
    "out  = z.mean()\n",
    "out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "``.requires_grad_( ... )`` changes an existing Tensor's ``requires_grad``\n",
    "flag in-place. The input flag defaults to ``False`` if not given.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad_(False)\n",
    "x.requires_grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Gradients\n",
    "---------\n",
    "Let's do a backpropagation now.\n",
    "Because ``out`` contains a single scalar, ``out.backward()`` is\n",
    "equivalent to ``out.backward(torch.tensor([value]))``.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "out.backward()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's print the gradients $\\frac{d(out)}{dx}$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "x.grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You should have got a matrix filled with  ``4.5``. Let’s call the ``out``\n",
    "*Tensor* “$o$”.\n",
    "We have that $o = \\frac{1}{4}\\sum_i z_i$,\n",
    "$z_i = 3(2x_i)^2$ and $z_i\\bigr\\rvert_{x_i=1} = 12$.\n",
    "Therefore,\n",
    "$\\frac{\\partial o}{\\partial x_i} = (6x_i)$, hence\n",
    "$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = 6 $.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can even calculate your own gradients and call `backward()` as:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 1.1369, -2.1690, -2.7972])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "\n",
    "y\n",
    "\n",
    "gradients = torch.randn([3], dtype=torch.float)\n",
    "y.backward(gradients)\n",
    "x.grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you no longer want autograd to track the history of tensor's  operations\n",
    "with ``.requires_grad=True`` , wrap the code block with the following command\n",
    "``with torch.no_grad()`` or use ``.detach()`` to remove the tensor from the computation graph:\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "x.requires_grad\n",
    "(x ** 2).requires_grad\n",
    "\n",
    "with torch.no_grad():\n",
    "\t(x ** 2).requires_grad\n",
    "\n",
    "x = x.detach()\n",
    "(x ** 2).requires_grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we are going  to calculate the gradients of the following equations on the next figure.\n",
    "All partial derivatives have been calculated using the chain rule and are also illustrated on the figure.\n",
    "\n",
    "![derivatives](images/chapter1_autograd.png)\n",
    "Now let's test with ``autograd`` if we calculated correctly all the  derivatives."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "tensor([14.])\n",
      "tensor([42.])\n",
      "tensor([28.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iliasprc/Documents/penvs/venv/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a = torch.tensor([1.], requires_grad=True)\n",
    "b = torch.tensor([2.], requires_grad=True)\n",
    "c = torch.tensor([3.], requires_grad=True)\n",
    "\n",
    "y = b*c\n",
    "u = y+a\n",
    "J = (u*u).sum()\n",
    "\n",
    "J.backward()\n",
    "\n",
    "for i in [y,u,a,b,c]:\n",
    "    print(i.grad)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see more details about the computational graph  in the following code"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "data: 1.0\n",
      "requires_grad: True\n",
      "grad: 2.0\n",
      "grad_fn: None\n",
      "is_leaf: True\n",
      "\n",
      "y\n",
      "data: 2.0\n",
      "requires_grad: True\n",
      "grad: 1.0\n",
      "grad_fn: None\n",
      "is_leaf: True\n",
      "\n",
      "z\n",
      "data: 2.0\n",
      "requires_grad: True\n",
      "grad: None\n",
      "grad_fn: <MulBackward0 object at 0x7f4c18c82cf8>\n",
      "is_leaf: False\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iliasprc/Documents/penvs/venv/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0, requires_grad = True)\n",
    "y = torch.tensor(2.0, requires_grad = True)\n",
    "z = x * y\n",
    "# Displaying\n",
    "z.backward()\n",
    "for i, name in zip([x, y, z], \"xyz\"):\n",
    "    print(f\"{name}\\ndata: {i.data}\\nrequires_grad: {i.requires_grad}\\n\\\n",
    "grad: {i.grad}\\ngrad_fn: {i.grad_fn}\\nis_leaf: {i.is_leaf}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Graph Visualization with Tensorboard\n",
    "\n",
    "\n",
    "Let's visualize the graph of a simple Linear Model.\n",
    "We used Tensorboard to visualize the following graph of a simple linear model.\n",
    "The computation graph is shown in the following Figure.\n",
    "\n",
    "![](images/autograd_chapter2_linear_graph.png)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "class Y(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Y, self).__init__()\n",
    "        self.y = torch.nn.Linear(3,3)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.y(x)\n",
    "        return out\n",
    "\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "m = Y()\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "writer.add_graph(m,x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this section, we described the main advantages of PyTorch's ``autograd``.\n",
    "\n",
    "**Read Later:**\n",
    "For more information read the documentation of ``autograd`` and ``Function`` is at\n",
    "- https://pytorch.org/docs/autograd\n",
    "- https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/\n",
    "\n",
    "\n",
    "In the next article, we will build and train our first neural network with PyTorch."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}