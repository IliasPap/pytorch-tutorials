{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "colab": {
   "name": "3_neural_networks_tutorial.ipynb",
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "vw6iY_35PiZl",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "#%matplotlib inline"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YIiFifyPiZq",
    "colab_type": "text"
   },
   "source": [
    "\n",
    "Neural Networks\n",
    "===============\n",
    "\n",
    "Use the torch.nn package to build a neural network.\n",
    "\n",
    "In the last lecture, I have already talked about `` autograd ``, `` nn ``package depends on `` autograd `` package to define the model and get derivative.\n",
    "An ``nn.Module`` contains each layer and a forward (input) method, which returns `` output``.\n",
    "\n",
    "E.g:\n",
    "\n",
    "![](https://pytorch.org/tutorials/_images/mnist.png)\n",
    "\n",
    "It is a simple feed-forward neural network that accepts an input, then passes it layer by layer, and finally outputs the result of the calculation.\n",
    "\n",
    "The typical training process of neural network is as follows:\n",
    "\n",
    "1. Define a neural network model containing some learnable parameters (or weights)\n",
    "2. Iterate over the dataset\n",
    "3. Process input through neural network\n",
    "4. Calculate the loss (the difference between the output and the correct value)\n",
    "5. Parameters of backpropagating the gradient back to the network\n",
    "6. Update the network parameters, mainly using the following simple update principle:\n",
    "``weight = weight - learning_rate * gradient``\n",
    "\n",
    "Create a network:\n",
    "------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2ICxuVMSPiZr",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "outputId": "ec752d3a-b4f6-48d9-d1d6-ba1aa6a4544d"
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 10 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 10, 5)\n",
    "        self.conv2 = nn.Conv2d(10, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wRQ_g1_PiZx",
    "colab_type": "text"
   },
   "source": [
    "The forward function must be defined in the model. The backward function (used to calculate the gradient) is automatically created by ``autograd``. You can use any operation for Tensor in the forward function.\n",
    "\n",
    "``net.parameters()`` returns a list and values of parameters (weights) that can be learned\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bUMnD_34PiZy",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "outputId": "e3516476-527e-4bce-bcab-e97d1585f7d5"
   },
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([10, 1, 5, 5])\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQzS0QMYPiZ2",
    "colab_type": "text"
   },
   "source": [
    "\n",
    "Note: The expected input size of this network (LeNet) is 32 × 32. If you use the MNIST dataset to train this network, please resize the image to 32 × 32.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d3axLz4-PiZ3",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "outputId": "eae85547-53dc-4407-dc59-e58b56556e7a"
   },
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "tensor([[-0.0315,  0.0076,  0.0138,  0.0684,  0.0567, -0.0410,  0.0404,  0.0657,\n",
      "         -0.0451, -0.0593]], grad_fn=<AddmmBackward>)\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exeUc_1ZPiZ6",
    "colab_type": "text"
   },
   "source": [
    "Clear the gradient buffer of all parameters to zero, and then perform the back propagation of the random gradient:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YS_hhwD1PiZ7",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "outputId": "73a8a3d5-7f6e-423b-ede4-6ca6a172983b"
   },
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))\n"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-10-7ef080f1cd99>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mnet\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mout\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrandn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m10\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, gradient, retain_graph, create_graph)\u001B[0m\n\u001B[1;32m    193\u001B[0m                 \u001B[0mproducts\u001B[0m\u001B[0;34m.\u001B[0m \u001B[0mDefaults\u001B[0m \u001B[0mto\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    194\u001B[0m         \"\"\"\n\u001B[0;32m--> 195\u001B[0;31m         \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    196\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    197\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001B[0m\n\u001B[1;32m     97\u001B[0m     Variable._execution_engine.run_backward(\n\u001B[1;32m     98\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad_tensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 99\u001B[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001B[0m\u001B[1;32m    100\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    101\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGI6dS4APiZ-",
    "colab_type": "text"
   },
   "source": [
    "## Note\n",
    " ``torch.nn`` only supports small batch input. The whole `` torch.nn``\n",
    "Packages only support small batch samples, not individual samples.\n",
    "\n",
    "For example, ``nn.Conv2d`` accepts a 4-dimensional tensor,\n",
    "  \n",
    "  ``Each dimension is numSamples * nChannels * Height * Width (number of samples * number of channels * height * width) ``.\n",
    "\n",
    "If you have a single sample, just use `` input.unsqueeze (0) `` to add other dimensions \n",
    "\n",
    "Before continuing, let's review the classes used so far.\n",
    "\n",
    "**review:**\n",
    "  * `` torch.Tensor``: a used multi-dimensional array * that automatically calls `` backward() `` to support automatic gradient calculation,\n",
    "      And save the *gradient* w.r.t about this vector.\n",
    "  * `` nn.Module``: neural network module. Package parameters, move to GPU, run, export, load, etc.\n",
    "  * `` nn.Parameter``: A variable, when it is assigned to a `` Module ``, it is *automatically registered as a parameter*.\n",
    "  * `` autograd.Function ``: To achieve the forward and reverse definition of an automatic derivation operation, each variable operation creates at least one function node, and each `` Tensor `` operation creates and receives one ``Tensor`` and the ``Function`` node of the function that encodes its history.\n",
    "\n",
    "**The key points are as follows:**\n",
    " \n",
    "\n",
    "*    Create a network\n",
    "*    Forward operation of input\n",
    "*    Calculate loss then backward operation\n",
    "*    Update network weights\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "Loss function\n",
    "-------------\n",
    "A loss function accepts a pair of (output, target) as input and calculates a value to estimate how much the network output differs from the target value.\n",
    "\n",
    "***Translator's Note: output is the output of the network, and target is the actual value***\n",
    "\n",
    "There are many different [loss functions] in the nn package (https://pytorch.org/docs/nn.html#loss-functions).\n",
    "`` nn.MSELoss `` is a relatively simple loss function, which calculates the **mean square error** between the output and the target,\n",
    "E.g:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4Q5TVnjdPiZ_",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "8870faf8-ac37-401a-eb3f-16244f28735f"
   },
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  \n",
    "target = target.view(1, -1)  \n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "tensor(0.9499, grad_fn=<MseLossBackward>)\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAvgGJ2uPiaD",
    "colab_type": "text"
   },
   "source": [
    "Now, if you follow `` loss`` in the reverse process, use its\n",
    "`` .grad_fn`` attribute, you will see the calculation diagram shown below.\n",
    "\n",
    "::\n",
    "\n",
    "     input-> conv2d-> relu-> maxpool2d-> conv2d-> relu-> maxpool2d\n",
    "           -> view-> linear-> relu-> linear-> relu-> linear\n",
    "           -> MSELoss\n",
    "           -> loss\n",
    "\n",
    "So, when we call `` loss.backward () ``, the entire calculation graph will be\n",
    "Differentiate according to loss, and all tensors in the figure set to `` requires_grad = True ``\n",
    "Will have a `` .grad `` tensor that accumulates with the gradient.\n",
    "\n",
    "To illustrate, let us take a few steps back:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YoCRFbM2PiaE",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "outputId": "28dce5eb-5557-40e3-cafc-55e69d9aac6b"
   },
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x7fae1aa25c88>\n",
      "<AddmmBackward object at 0x7fae1aa25cc0>\n",
      "<AccumulateGrad object at 0x7fae1aa25c88>\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhSbwvIQPiaH",
    "colab_type": "text"
   },
   "source": [
    "Back propagation\n",
    "--------\n",
    "Call ``loss.backward()`` to get the error of back propagation.\n",
    "\n",
    "However, you need to clear the existing gradient before calling, otherwise the gradient will be accumulated to the existing gradient.\n",
    "\n",
    "Now, we will call ``loss.backward()`` and look at the gradient of the bias term of the conv1 layer before and after back propagation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NpSNuUWVPiaH",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "outputId": "2ea4222a-36a6-4c23-9996-cc63cdd837ee"
   },
   "source": [
    "net.zero_grad()     \n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([ 0.0064,  0.0044, -0.0034, -0.0062, -0.0086, -0.0061, -0.0030,  0.0063,\n",
      "        -0.0017,  0.0067])\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzius6-7PiaM",
    "colab_type": "text"
   },
   "source": [
    "How to use the loss function\n",
    "\n",
    "**Read later:**\n",
    "\n",
    "   The `nn` package contains various modules and loss functions used to form the building blocks of deep neural networks. For complete documentation, please see [here] (https://pytorch.org/docs/nn).\n",
    "\n",
    "\n",
    "\n",
    "Update weights\n",
    "------------------\n",
    "In practice, the simplest weight update rule is stochastic gradient descent (SGD):\n",
    "\n",
    "      `` weight = weight-learning_rate * gradient ``\n",
    "\n",
    "We can implement this rule using simple Python code:\n",
    "\n",
    "```python\n",
    "\n",
    "learning_rate = 0.01\n",
    "for f in net.parameters ():\n",
    "     f.data.sub_ (f.grad.data * learning_rate)\n",
    "``` \n",
    "But when using a neural network to use various update rules, such as SGD, Nesterov-SGD, Adam, RMSPROP, etc., a package `` torch.optim `` is built in PyTorch to implement all these rules.\n",
    "Using them is very simple:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "t0Tk6cuXPiaM",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NT53M6PwPiaP",
    "colab_type": "text"
   },
   "source": [
    "**Note:** \n",
    "    \n",
    "Observe how to use ``optimizer.zero_grad ()`` to manually set the gradient buffer to zero. This is because the gradient is accumulated as described in the Backprop section.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IgflFG2gPiaQ",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    ""
   ],
   "execution_count": 0,
   "outputs": []
  }
 ]
}